# 阿里云手册



## 阿里云备案



* 需要备案服务号：ECS有效期必需>=3个月，必须有公网带宽，必须有EIP
* 备案服务号：如果没有，可以从另外渠道获取，只是一个备案通行证。
* IP/公安备案：通过ICP备案后需要IP或者公安备案后才算真正的正常提供服务，ICP备案完成后，网站提供服务后30日内完成公安备案。



## 添加公网网卡

1. 创建弹性网卡 

2. 弹性IP绑定辅助弹性网卡

3. 到新创建的弹性网卡上进行绑定ECS 

4. 到绑定弹性IP的ECS上创建ifcfg-eth1配置文件并重启网络服务，实现添加公网地址网卡，此时会卡住，因为此时公网网卡已经生效，需要连接公网地址进入管理

   ```bash
   [root@iptables network-scripts]# cp ifcfg-eth0 ifcfg-eth1
   # 此网卡必须绑定第二个弹性网卡后才能使用
   [root@iptables network-scripts]# cat ifcfg-eth1
   BOOTPROTO=dhcp
   DEVICE=eth1
   ONBOOT=yes
   STARTMODE=auto
   TYPE=Ethernet
   USERCTL=no
   [root@iptables network-scripts]#systemctl restart network
   [root@iptables network-scripts]#ssh root@publicIP
   ```




## 阿里云自定义DNS配置



[如何在Linux实例中自定义配置DNS-阿里云帮助中心 (aliyun.com)](https://help.aliyun.com/zh/ecs/how-do-i-customize-the-dns-settings-of-a-linux-instance?spm=a2c4g.11186623.0.i12)



### 方式一 使用DHCP选项集

[使用DHCP选项集](https://help.aliyun.com/zh/vpc/user-guide/work-with-dhcp-options-sets)
通过DHCP选项集功能，您可以为VPC中的ECS实例配置DNS服务器IP地址和域名。

**实测生效**

```bash
# 配置DHCP选项集前
[ecs-assist-user@elasticsearch002 ~]$ cat /etc/resolv.conf 
; generated by /usr/sbin/dhclient-script
nameserver 100.100.2.136
nameserver 100.100.2.138

# 配置DHCP选项集后

# 重启ECS实例/重启DHCP Client服务/重启网络服务，使DHCP client重新获取DHCP配置
[ecs-assist-user@elasticsearch002 ~]$ sudo systemctl restart network

[ecs-assist-user@elasticsearch002 ~]$ cat /etc/resolv.conf 
; generated by /usr/sbin/dhclient-script
search hs.com
nameserver 114.114.114.114
nameserver 100.100.2.138
nameserver 100.100.2.136
```








### 方式二 配置cloud服务

**CentOS7实测未生效**

**Ubuntu 18实测生效**



#### 所有Linux系统操作

**关闭cloud-init自动配置网络**

```bash
# 在# Example datasource config内容上增加以下配置，关闭cloud-init中的自动配置网络的参数，避免网卡配置文件内容被覆盖。
vim /etc/cloud/cloud.cfg
network:
  config: disabled
```



#### Ubuntu 18/20/22

```bash
# 在文件底部添加以下配置，自定义配置DNS信息。
vim /etc/netplan/50-cloud-init.yaml
  nameservers:
    addresses:[223.5.XX.XX,223.6.XX.XX]

# 应用网络配置
netplan apply

# 建立/run/systemd/resolve/resolv.conf文件的软链接，避免因为/etc/resolv.conf配置文件内容更新而导致配置还原。
sudo ln -snf /run/systemd/resolve/resolv.conf /etc/resolv.conf


# ansible
# shell,‘$’是特殊字符，在命令行需要用反斜杠，\$，playbook则不需要
ansible 172.16.0.13 -m shell -a "grep -A 100 'nameservers' /etc/netplan/50-cloud-init.yaml | grep '172.16.0.10' || sed -i '\$a\            nameservers:\n                search:\n                - hs.com\n                addresses: [172.16.0.10,100.100.2.136]' /etc/netplan/50-cloud-init.yaml && netplan apply && ln -snf /run/systemd/resolve/resolv.conf /etc/resolv.conf"
```



#### CentOS 6/7/8

```bash
# 关闭cloud-init中的自动配置网络的参数，见上面所有系统操作

# 为了便于维护，统一通过网络服务来管理网络和DNS配置，即DNS也配置在网卡配置文件中。
vim /etc/sysconfig/network-scripts/ifcfg-eth0
PEERDNS=no
DNS1=[10.10.10.240]
DNS2=[100.100.2.136]

# centos7
systemctl restart network

# centos8
nmcli c reload
nmcli d connect eth0
```



### 方式三 rc.local配置

```bash
# 开机自启
[root@nginx named]# sudo cat /etc/rc.d/rc.local
# auto set custom DNS Server.
echo 'search hs.com' > /etc/resolv.conf
echo 'nameserver 10.10.10.240' >> /etc/resolv.conf
echo 'nameserver 100.100.2.136' >> /etc/resolv.conf
```



### 方式四 windows配置

1. 手动编辑网卡配置，配置dns地址。
2. 重启ECS/重启网络服务生效。







## 在线硬盘扩容

```bash
# 确认分区表格式和文件系统
[root@jumpserver ~]# fdisk -lu /dev/vda 

Disk /dev/vda: 214.7 GB, 214748364800 bytes, 419430400 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk label type: dos
Disk identifier: 0x000e0364

   Device Boot      Start         End      Blocks   Id  System
/dev/vda1   *        2048   419430366   209714159+  83  Linux
# 如果System为Linux，说明数据盘使用的是MBR分区表格式。
# 如果System为GPT，说明数据盘使用的是GPT分区表格式。

[root@jumpserver ~]# blkid /dev/vda1 
/dev/vda1: UUID="795c9f31-f638-4308-8fab-4a7d5c606beb" TYPE="ext4" 

# 运行以下命令确认文件系统的状态
# ext*文件系统：
[root@jumpserver ~]# e2fsck -n /dev/vda1 
e2fsck 1.42.9 (28-Dec-2013)
Warning!  /dev/vda1 is mounted.
Warning: skipping journal recovery because doing a read-only filesystem check.
/dev/vda1: clean, 258247/13107200 files, 4033360/52428539 blocks
# xfs文件系统：
xfs_repair -n /dev/vda1
#注意 本示例中，文件系统状态为clean，表示文件系统状态正常。如果状态不是clean，请排查并修复。

# 阿里云在线扩容后容量 
[root@jumpserver ~]# fdisk -lu /dev/vda 

Disk /dev/vda: 225.5 GB, 225485783040 bytes, 440401920 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk label type: dos
Disk identifier: 0x000e0364

   Device Boot      Start         End      Blocks   Id  System
/dev/vda1   *        2048   419430366   209714159+  83  Linux

# 在线扩容云盘后，使用growpart和resize2fs等工具完成Linux系统盘扩展分区和文件系统
[root@jumpserver ~]# yum install cloud-utils-growpart xfsprogs -y
[root@jumpserver ~]# df -Th
Filesystem     Type      Size  Used Avail Use% Mounted on
devtmpfs       devtmpfs  3.7G     0  3.7G   0% /dev
tmpfs          tmpfs     3.7G  220K  3.7G   1% /dev/shm
tmpfs          tmpfs     3.7G  640K  3.7G   1% /run
tmpfs          tmpfs     3.7G     0  3.7G   0% /sys/fs/cgroup
/dev/vda1      ext4      197G   13G  177G   7% /
overlay        overlay   197G   13G  177G   7% /var/lib/docker/overlay2/3ac74b0f19183813328a7ad7f9177b55504a296168b6451ffe60462108fd45e8/merged
tmpfs          tmpfs     756M     0  756M   0% /run/user/0
# 返回分区（/dev/vda1）容量是197GiB，文件系统类型为ext4

# 运行以下命令扩容分区
[root@jumpserver ~]# growpart /dev/vda 1 
CHANGED: partition=1 start=2048 old: size=419428319 end=419430367 new: size=440399839 end=440401887
[root@jumpserver ~]# fdisk -lu /dev/vda 

Disk /dev/vda: 225.5 GB, 225485783040 bytes, 440401920 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk label type: dos
Disk identifier: 0x000e0364

   Device Boot      Start         End      Blocks   Id  System
/dev/vda1   *        2048   440401886   220199919+  83  Linux    --end扇区已经扩展了

# lVM 卷操作
sudo pvresize /dev/vdb1
sudo lvextend -L +100G /dev/data/elasticsearch
#

# 扩展文件系统
# ext*文件系统（例如ext3和ext4）：运行以下命令扩展文件系统。
[root@jumpserver ~]# resize2fs /dev/vda1   --扩展文件系统
resize2fs 1.42.9 (28-Dec-2013)
Filesystem at /dev/vda1 is mounted on /; on-line resizing required
old_desc_blocks = 13, new_desc_blocks = 14
The filesystem on /dev/vda1 is now 55049979 blocks long.
# 成功后大小
[root@jumpserver ~]# df -Th
Filesystem     Type      Size  Used Avail Use% Mounted on
devtmpfs       devtmpfs  3.7G     0  3.7G   0% /dev
tmpfs          tmpfs     3.7G  240K  3.7G   1% /dev/shm
tmpfs          tmpfs     3.7G  644K  3.7G   1% /run
tmpfs          tmpfs     3.7G     0  3.7G   0% /sys/fs/cgroup
/dev/vda1      ext4      207G   13G  186G   7% /
overlay        overlay   207G   13G  186G   7% /var/lib/docker/overlay2/3ac74b0f19183813328a7ad7f9177b55504a296168b6451ffe60462108fd45e8/merged
tmpfs          tmpfs     756M     0  756M   0% /run/user/0

# xfs文件系统：运行以下命令扩展文件系统。
xfs_growfs <mountpoint>

example: xfs_growfs /
```





## SNAT和DNAT

1. 阿里云VPC网络IP地址不可以自己设定，网关不可以设定。
   ip地址只能在控制台设定(只能关机状态下更改私网IP)，网关是在VPC网络的路由表中建立的默认路由，指定下一跳到ECS(代理服务器)

2. 网关到达绑定弹性IP的ECS，此时这个ECS需要做SNAT转换

3. 首先要开启ip转发

   ```bash
   [root@iptables ~]# cat /etc/sysctl.d/snat.conf 
   net.ipv4.ip_forward = 1
   # 应用所有内核相关配置
   [root@iptables ~]# sysctl --system
   [root@iptables ~]# sysctl -a | grep "net.ipv4.ip_forward"
   net.ipv4.ip_forward = 1
   ```

4. 创建iptables规则

   ```bash
   [root@iptables yum.repos.d]# yum install -y iptables-services
   #input
   [root@iptables yum.repos.d]# iptables -I INPUT -s 222.66.21.210 -p tcp --dport 9572 -j ACCEPT
   [root@iptables yum.repos.d]# iptables -I INPUT 2 -p tcp --dport 9572 -j DROP
   [root@iptables yum.repos.d]# iptables -I INPUT -i lo -j ACCEPT
   [root@iptables yum.repos.d]# iptables -I INPUT -p icmp -j ACCEPT
   [root@iptables yum.repos.d]# iptables -I INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT
   #snat
   [root@iptables yum.repos.d]# iptables -t nat -A POSTROUTING -s 10.0.0.0/8 -o eth1 -j SNAT --to-source 47.100.73.115
   #dnat
   [root@iptables yum.repos.d]# iptables -t nat -I PREROUTING -d 47.100.73.115 -p tcp --dport 80 --destication-to 10.10.10.240:80 
   #start boot
   [root@iptables yum.repos.d]# systemctl enable iptables
   [root@iptables yum.repos.d]# service iptables save
   ```

    

## Openvpn

### 1. 部署

```bash
[root@iptables download]# yum install -y openvpn
[root@iptables download]# wget https://github.com/OpenVPN/easy-rsa/releases/download/v3.0.8/EasyRSA-3.0.8.tgz
--配置服务端证书
[root@iptables /download]# tar xf EasyRSA-3.0.8.tgz
[root@iptables /download]# cp -a EasyRSA-3.0.8 /etc/openvpn/server
[root@iptables /download]# cd /etc/openvpn/server/EasyRSA-3.0.8/
[root@iptables /etc/openvpn/server/EasyRSA-3.0.8]# cp vars.example vars
[root@iptables /etc/openvpn/server/EasyRSA-3.0.8]# vim vars
set_var EASYRSA_REQ_COUNTRY     "CN"
set_var EASYRSA_REQ_PROVINCE    "Shanghai"
set_var EASYRSA_REQ_CITY        "Shanghai"
set_var EASYRSA_REQ_ORG         "ops"
set_var EASYRSA_REQ_EMAIL       "jack@qq.com"
set_var EASYRSA_REQ_OU          "OpenVPN"
set_var EASYRSA_CERT_EXPIRE     3650        --开启客户端证书有效期
[root@iptables /etc/openvpn/server/EasyRSA-3.0.8]# ./easyrsa init-pki   --输入yes初始化PKI
[root@iptables /etc/openvpn/server/EasyRSA-3.0.8]# ./easyrsa build-ca
Enter New CA Key Passphrase:             --输入自定义密码 
Re-Enter New CA Key Passphrase:         --输入自定义密码 
Common Name (eg: your user, host, or server name) [Easy-RSA CA]:domain.com       --通用名称
CA creation complete and you may now import and sign cert requests.
Your new CA certificate file for publishing is at:
/etc/openvpn/server/EasyRSA-3.0.8/pki/ca.crt

[root@iptables /etc/openvpn/server/EasyRSA-3.0.8]# ./easyrsa gen-req server nopass    --创建服务器证书名称叫server
Common Name (eg: your user, host, or server name) [server]:openvpn.domain.com
Keypair and certificate request completed. Your files are:
req: /etc/openvpn/server/EasyRSA-3.0.8/pki/reqs/server.req
key: /etc/openvpn/server/EasyRSA-3.0.8/pki/private/server.key

[root@iptables /etc/openvpn/server/EasyRSA-3.0.8]# ./easyrsa sign server server     --签署服务器证书类型，这个服务器证书名称为server
Request subject, to be signed as a server certificate for 825 days:
subject=
    commonName                = openvpn.domain.com
Type the word 'yes' to continue, or any other input to abort.
  Confirm request details: yes
Enter pass phrase for /etc/openvpn/server/EasyRSA-3.0.8/pki/private/ca.key:      --输入ca的密码
Certificate created at: /etc/openvpn/server/EasyRSA-3.0.8/pki/issued/server.crt

[root@iptables /etc/openvpn/server/EasyRSA-3.0.8]# ./easyrsa gen-dh   --生成DH密钥交换算法文件
DH parameters of size 2048 created at /etc/openvpn/server/EasyRSA-3.0.8/pki/dh.pem


--配置客户端证书
[root@iptables /download]# cd /etc/openvpn/client/
[root@iptables /etc/openvpn/client]# cp -ar /download/EasyRSA-3.0.8 .
[root@iptables /etc/openvpn/client]# ll
total 0
drwxrwx--- 4 root openvpn 214 Sep 10  2020 EasyRSA-3.0.8
[root@iptables /etc/openvpn/client]# cd /etc/openvpn/client/EasyRSA-3.0.8/
[root@iptables /etc/openvpn/client/EasyRSA-3.0.8]# ./easyrsa init-pki
[root@iptables /etc/openvpn/client/EasyRSA-3.0.8]# ./easyrsa gen-req client   --生成客户端证书及请求
writing new private key to '/etc/openvpn/client/EasyRSA-3.0.8/pki/easy-rsa-18233.0QLwEq/tmp.N7mIEz'
Enter PEM pass phrase:                     --输入自定义密码 
Verifying - Enter PEM pass phrase:    --输入自定义密码 
Common Name (eg: your user, host, or server name) [client]:     --输入通用名称，默认为证书名称
Keypair and certificate request completed. Your files are:
req: /etc/openvpn/client/EasyRSA-3.0.8/pki/reqs/client.req
key: /etc/openvpn/client/EasyRSA-3.0.8/pki/private/client.key

--服务端导入客户端的请求文件
[root@iptables /etc/openvpn/client/EasyRSA-3.0.8]# cd /etc/openvpn/server/EasyRSA-3.0.8/
[root@iptables /etc/openvpn/server/EasyRSA-3.0.8]# ./easyrsa import-req /etc/openvpn/client/EasyRSA-3.0.8/pki/reqs/client.req client   --导入客户端表示文件并命名为client
--签署客户端请求
[root@iptables /etc/openvpn/server/EasyRSA-3.0.8]# ./easyrsa sign client client   --签署类型为客户端类型，客户端请求名称为client
Type the word 'yes' to continue, or any other input to abort.
  Confirm request details: yes
Enter pass phrase for /etc/openvpn/server/EasyRSA-3.0.8/pki/private/ca.key:     --输入ca密码
Certificate created at: /etc/openvpn/server/EasyRSA-3.0.8/pki/issued/client.crt


--把服务器端私钥、公钥、根证书、DH算法文件放到etc/openvpn/ 目录下   
[root@iptables /etc/openvpn/server/EasyRSA-3.0.8]# cp /etc/openvpn/server/EasyRSA-3.0.8/pki/ca.crt /etc/openvpn/
[root@iptables /etc/openvpn/server/EasyRSA-3.0.8]# cp /etc/openvpn/server/EasyRSA-3.0.8/pki/dh.pem /etc/openvpn/
[root@iptables /etc/openvpn/server/EasyRSA-3.0.8]# cp /etc/openvpn/server/EasyRSA-3.0.8/pki/private/server.key /etc/openvpn/
[root@iptables /etc/openvpn/server/EasyRSA-3.0.8]# cp /etc/openvpn/server/EasyRSA-3.0.8/pki/issued/server.crt /etc/openvpn/
[root@iptables /etc/openvpn/server/EasyRSA-3.0.8]# ll /etc/openvpn/
total 20
-rw------- 1 root root    1204 Mar 18 11:15 ca.crt
drwxr-x--- 3 root openvpn   27 Mar 18 10:57 client
-rw------- 1 root root     424 Mar 18 11:15 dh.pem
drwxr-x--- 2 root openvpn    6 Dec 10 00:57 server
-rw------- 1 root root    4644 Mar 18 11:15 server.crt
-rw------- 1 root root    1704 Mar 18 11:15 server.key
--把客户端私钥、公钥、根证书放到root/openvpn/client 目录下
[root@iptables /etc/openvpn/server/EasyRSA-3.0.8]# cp /etc/openvpn/client/EasyRSA-3.0.8/pki/private/client.key /etc/openvpn/client/
[root@iptables /etc/openvpn/server/EasyRSA-3.0.8]# cp /etc/openvpn/server/EasyRSA-3.0.8/pki/ca.crt /etc/openvpn/client/
[root@iptables /etc/openvpn/server/EasyRSA-3.0.8]# cp /etc/openvpn/server/EasyRSA-3.0.8/pki/issued/client.crt /etc/openvpn/client/
[root@iptables /etc/openvpn/server/EasyRSA-3.0.8]# ll /etc/openvpn/client/
total 16
-rw------- 1 root root    1204 Mar 18 11:17 ca.crt
-rw------- 1 root root    4472 Mar 18 11:17 client.crt
-rw------- 1 root root    1834 Mar 18 11:17 client.key
drwxrwx--- 5 root openvpn  225 Mar 18 10:59 EasyRSA-3.0.8

--在服务器端配置文件
[root@iptables /etc/openvpn]# rpm -ql openvpn | grep server.conf
/usr/share/doc/openvpn-2.4.10/sample/sample-config-files/roadwarrior-server.conf
/usr/share/doc/openvpn-2.4.10/sample/sample-config-files/server.conf
/usr/share/doc/openvpn-2.4.10/sample/sample-config-files/xinetd-server-config
[root@iptables /etc/openvpn]# cp /usr/share/doc/openvpn-2.4.10/sample/sample-config-files/server.conf /etc/openvpn/
[root@iptables /etc/openvpn]# mkdir -p /var/log/openvpn
[root@iptables /etc/openvpn]# chown -R root.openvpn /etc/openvpn/
[root@iptables /etc/openvpn]# chown -R root.openvpn /var/log/openvpn/
[root@iptables /etc/openvpn]# chmod -R 770 /etc/openvpn/
[root@iptables /etc/openvpn]# chmod -R 770 /var/log/openvpn/
[root@iptables /etc/openvpn]# vim /etc/openvpn/server.conf 
[root@iptables /etc/openvpn]# grep '^[^#|;]' /etc/openvpn/server.conf   
--------此配置是客户端证书认证---------
local 0.0.0.0
port 1194
proto tcp
dev tun
ca /etc/openvpn/ca.crt
cert /etc/openvpn/server.crt
key /etc/openvpn/server.key  # This file should be kept secret
dh /etc/openvpn/dh.pem
server 192.168.177.0 255.255.255.0
ifconfig-pool-persist ipp.txt
push "redirect-gateway def1 bypass-dhcp"
push "dhcp-option DNS 223.6.6.6"
client-to-client
keepalive 10 120
comp-lzo
max-clients 100
user openvpn
group openvpn
persist-key
persist-tun
status /var/log/openvpn/openvpn-status.log
log         /var/log/openvpn/openvpn.log

verb 3
-----------------------------

-------此配置是客户端密码认证，建议使用这个-------
local 0.0.0.0
port 1194
proto tcp
dev tun
ca /etc/openvpn/ca.crt
cert /etc/openvpn/server.crt
key /etc/openvpn/server.key  # This file should be kept secret
dh /etc/openvpn/dh.pem
server 192.168.177.0 255.255.255.0
ifconfig-pool-persist ipp.txt
push "redirect-gateway def1 bypass-dhcp"
push "dhcp-option DNS 223.6.6.6"
client-to-client
keepalive 10 120
comp-lzo
max-clients 100
user openvpn
group openvpn
persist-key
persist-tun
status /var/log/openvpn/openvpn-status.log
log         /var/log/openvpn/openvpn.log
verb 3
auth-user-pass-verify /etc/openvpn/checkpsw.sh via-env
client-cert-not-required

script-security 3 
-----------------------------

----------or-客户端证书和密码认证------------
local 0.0.0.0
port 1194
proto tcp
dev tun
ca /etc/openvpn/ca.crt
cert /etc/openvpn/server.crt
key /etc/openvpn/server.key  # This file should be kept secret
dh /etc/openvpn/dh.pem
server 192.168.177.0 255.255.255.0
ifconfig-pool-persist ipp.txt
push "redirect-gateway def1 bypass-dhcp"
push "dhcp-option DNS 223.6.6.6"
client-to-client
keepalive 10 120
comp-lzo
max-clients 100
user openvpn
group openvpn
persist-key
persist-tun
status /var/log/openvpn/openvpn-status.log
log         /var/log/openvpn/openvpn.log
verb 3
auth-user-pass-verify /etc/openvpn/checkpsw.sh via-env
username-as-common-name

script-security 3
-------------------------------

[root@iptables /etc/openvpn]# chmod 777 checkpsw.sh
[root@iptables /etc/openvpn]# cat checkpsw.sh
#!/bin/bash 
########################################################### 

# checkpsw.sh (C) 2004 Mathias Sundman <mathias@openvpn.se> 

# 

# This script will authenticate OpenVpn users against 

# a plain text file. The passfile should simply contain 

# one row per user with the username first followed by 

# one or more space(s) or tab(s) and then the password.

PASSFILE="/etc/openvpn/test.key"
LOG_FILE="/var/log/openvpn/test.key.log"
TIME_STAMP=`date "+%Y-%m-%d %T"`

###########################################################

if [ ! -r "${PASSFILE}" ]; then
echo "${TIME_STAMP}: Could not open password file \"${PASSFILE}\" for reading." >> ${LOG_FILE}
exit 1
fi

CORRECT_PASSWORD=`awk '!/^;/&&!/^#/&&$1=="'${username}'"{print $2;exit}' ${PASSFILE}`

if [ "${CORRECT_PASSWORD}" = "" ]; then
echo "${TIME_STAMP}: User does not exist: username=\"${username}\", password=\"${password}\"." >> ${LOG_FILE}
exit 1
fi

if [ "${password}" = "${CORRECT_PASSWORD}" ]; then
echo "${TIME_STAMP}: Successful authentication: username=\"${username}\"." >> ${LOG_FILE}
exit 0
fi

echo "${TIME_STAMP}: Incorrect password: username=\"${username}\", password=\"${password}\"." >> ${LOG_FILE}
exit 1
####################################

[root@iptables /etc/openvpn]# chmod 444 test.key

[root@iptables /etc/openvpn]# cat test.key 
---

user password
---

[root@iptables /etc/openvpn]# cat /usr/lib/systemd/system/openvpn-PREFIX.service 
-------

[Unit]
Description=OpenVPN service
After=network-online.target

[Service]
User=root
Group=root
Type=simple
ExecStart=/usr/sbin/openvpn --config /etc/openvpn/server.conf
Restart=on-failure

[Install]

WantedBy=multi-user.target
------

[root@iptables /etc/openvpn]# systemctl daemon-reload
[root@iptables /etc/openvpn]# systemctl start openvpn-PREFIX
[root@iptables /etc/openvpn]# systemctl enable openvpn-PREFIX
[root@iptables /etc/openvpn]# iptables -t nat -A POSTROUTING -s 192.168.177.0/24 -j MASQUERADE
[root@iptables /etc/openvpn]# iptables -I INPUT 4 -p tcp --dport 1194 -j ACCEPT


#客户端配置

[root@iptables /etc/openvpn]# vim client.ovpn
------客户端证书认证，如果证书未设密码则用户不用密码即可连接------
client
dev tun
proto tcp
remote 47.100.73.115 1194
resolv-retry infinite
nobind
persist-key
persist-tun
ca ca.crt
cert client.crt
key client.key
cipher AES-256-CBC
comp-lzo
verb 3
----客户端密码认证，不需要客户端公私钥，建议使用这个------
client
dev tun
proto tcp
remote 47.100.73.115 1194
resolv-retry infinite
nobind
persist-key
persist-tun
ca ca.crt
auth-user-pass
cipher AES-256-CBC
comp-lzo
verb 3
----客户端证书和密码认证，如果证书也设置密码，则需要输入两次密码，一次是证书密码，一次是密码认证的密码---
client
dev tun
proto tcp
remote 47.100.73.115 1194
resolv-retry infinite
nobind
persist-key
persist-tun
ca ca.crt
cert client.crt
key client.key
auth-user-pass
cipher AES-256-CBC
comp-lzo

verb 3
------------------------

客户端看情况增加路由才能访问内部网络：
route add 192.168.13.0 mask 255.255.255.0 172.168.2.254
route add 192.168.10.0 mask 255.255.255.0 172.168.2.254
```



### 2. 通过公网地址端口映射

1. 通过防火墙 `公网IP：Port`  -> `私网IP：Port` 
2. OpenVPN Client 配置文件更改为`公网IP：Port`
3. 客户端连接时，跟连接`私网IP：Port` 一样成功连接

```bash
Wed Apr 10 17:42:40 2024 OpenVPN 2.4.7 x86_64-w64-mingw32 [SSL (OpenSSL)] [LZO] [LZ4] [PKCS11] [AEAD] built on Feb 21 2019
Wed Apr 10 17:42:40 2024 Windows version 6.2 (Windows 8 or greater) 64bit
Wed Apr 10 17:42:40 2024 library versions: OpenSSL 1.1.0j  20 Nov 2018, LZO 2.10
Enter Management Password:
Wed Apr 10 17:42:40 2024 MANAGEMENT: TCP Socket listening on [AF_INET]127.0.0.1:25340
Wed Apr 10 17:42:40 2024 Need hold release from management interface, waiting...
Wed Apr 10 17:42:41 2024 MANAGEMENT: Client connected from [AF_INET]127.0.0.1:25340
Wed Apr 10 17:42:41 2024 MANAGEMENT: CMD 'state on'
Wed Apr 10 17:42:41 2024 MANAGEMENT: CMD 'log all on'
Wed Apr 10 17:42:41 2024 MANAGEMENT: CMD 'echo all on'
Wed Apr 10 17:42:41 2024 MANAGEMENT: CMD 'bytecount 5'
Wed Apr 10 17:42:41 2024 MANAGEMENT: CMD 'hold off'
Wed Apr 10 17:42:41 2024 MANAGEMENT: CMD 'hold release'
Wed Apr 10 17:42:42 2024 MANAGEMENT: CMD 'username "Auth" "test123"'
Wed Apr 10 17:42:42 2024 MANAGEMENT: CMD 'password [...]'
Wed Apr 10 17:42:42 2024 WARNING: No server certificate verification method has been enabled.  See http://openvpn.net/howto.html#mitm for more info.
Wed Apr 10 17:42:42 2024 TCP/UDP: Preserving recently used remote address: [AF_INET]58.246.78.150:2094
Wed Apr 10 17:42:42 2024 Socket Buffers: R=[65536->65536] S=[65536->65536]
Wed Apr 10 17:42:42 2024 Attempting to establish TCP connection with [AF_INET]58.246.78.150:2094 [nonblock]
Wed Apr 10 17:42:42 2024 MANAGEMENT: >STATE:1712742162,TCP_CONNECT,,,,,,
Wed Apr 10 17:42:43 2024 TCP connection established with [AF_INET]58.246.78.150:2094
Wed Apr 10 17:42:43 2024 TCP_CLIENT link local: (not bound)
Wed Apr 10 17:42:43 2024 TCP_CLIENT link remote: [AF_INET]58.246.78.150:2094
Wed Apr 10 17:42:43 2024 MANAGEMENT: >STATE:1712742163,WAIT,,,,,,
Wed Apr 10 17:42:43 2024 MANAGEMENT: >STATE:1712742163,AUTH,,,,,,
Wed Apr 10 17:42:43 2024 TLS: Initial packet from [AF_INET]58.246.78.150:2094, sid=37b85cf0 377d91af
Wed Apr 10 17:42:43 2024 WARNING: this configuration may cache passwords in memory -- use the auth-nocache option to prevent this
Wed Apr 10 17:42:43 2024 VERIFY OK: depth=1, CN=markli.cn
Wed Apr 10 17:42:43 2024 VERIFY OK: depth=0, CN=openvpn.markli.cn
Wed Apr 10 17:42:43 2024 WARNING: 'link-mtu' is used inconsistently, local='link-mtu 1560', remote='link-mtu 1544'
Wed Apr 10 17:42:43 2024 WARNING: 'cipher' is used inconsistently, local='cipher AES-256-CBC', remote='cipher BF-CBC'
Wed Apr 10 17:42:43 2024 WARNING: 'keysize' is used inconsistently, local='keysize 256', remote='keysize 128'
Wed Apr 10 17:42:43 2024 Control Channel: TLSv1.2, cipher TLSv1.2 ECDHE-RSA-AES256-GCM-SHA384, 2048 bit RSA
Wed Apr 10 17:42:43 2024 [openvpn.markli.cn] Peer Connection Initiated with [AF_INET]58.246.78.150:2094
Wed Apr 10 17:42:45 2024 MANAGEMENT: >STATE:1712742165,GET_CONFIG,,,,,,
Wed Apr 10 17:42:45 2024 SENT CONTROL [openvpn.markli.cn]: 'PUSH_REQUEST' (status=1)
Wed Apr 10 17:42:45 2024 PUSH: Received control message: 'PUSH_REPLY,redirect-gateway def1 bypass-dhcp,dhcp-option DNS 223.6.6.6,route 192.168.179.0 255.255.255.0,topology net30,ping 10,ping-restart 120,ifconfig 192.168.179.10 192.168.179.9,peer-id 0,cipher AES-256-GCM'
Wed Apr 10 17:42:45 2024 OPTIONS IMPORT: timers and/or timeouts modified
Wed Apr 10 17:42:45 2024 OPTIONS IMPORT: --ifconfig/up options modified
Wed Apr 10 17:42:45 2024 OPTIONS IMPORT: route options modified
Wed Apr 10 17:42:45 2024 OPTIONS IMPORT: --ip-win32 and/or --dhcp-option options modified
Wed Apr 10 17:42:45 2024 OPTIONS IMPORT: peer-id set
Wed Apr 10 17:42:45 2024 OPTIONS IMPORT: adjusting link_mtu to 1627
Wed Apr 10 17:42:45 2024 OPTIONS IMPORT: data channel crypto options modified
Wed Apr 10 17:42:45 2024 Data Channel: using negotiated cipher 'AES-256-GCM'
Wed Apr 10 17:42:45 2024 Outgoing Data Channel: Cipher 'AES-256-GCM' initialized with 256 bit key
Wed Apr 10 17:42:45 2024 Incoming Data Channel: Cipher 'AES-256-GCM' initialized with 256 bit key
Wed Apr 10 17:42:45 2024 interactive service msg_channel=0
Wed Apr 10 17:42:45 2024 ROUTE_GATEWAY 172.168.2.254/255.255.255.0 I=14 HWADDR=b8:ca:3a:ba:66:ba
Wed Apr 10 17:42:45 2024 open_tun
Wed Apr 10 17:42:45 2024 TAP-WIN32 device [以太网 2] opened: \\.\Global\{1596C74F-B5B8-415C-B0CF-88AB4A480FAE}.tap
Wed Apr 10 17:42:45 2024 TAP-Windows Driver Version 9.21 
Wed Apr 10 17:42:45 2024 Notified TAP-Windows driver to set a DHCP IP/netmask of 192.168.179.10/255.255.255.252 on interface {1596C74F-B5B8-415C-B0CF-88AB4A480FAE} [DHCP-serv: 192.168.179.9, lease-time: 31536000]
Wed Apr 10 17:42:45 2024 Successful ARP Flush on interface [40] {1596C74F-B5B8-415C-B0CF-88AB4A480FAE}
Wed Apr 10 17:42:45 2024 MANAGEMENT: >STATE:1712742165,ASSIGN_IP,,192.168.179.10,,,,
Wed Apr 10 17:42:50 2024 TEST ROUTES: 2/2 succeeded len=1 ret=1 a=0 u/d=up
Wed Apr 10 17:42:50 2024 C:\Windows\system32\route.exe ADD 58.246.78.150 MASK 255.255.255.255 172.168.2.254
Wed Apr 10 17:42:50 2024 ROUTE: CreateIpForwardEntry succeeded with dwForwardMetric1=35 and dwForwardType=4
Wed Apr 10 17:42:50 2024 Route addition via IPAPI succeeded [adaptive]
Wed Apr 10 17:42:50 2024 C:\Windows\system32\route.exe ADD 0.0.0.0 MASK 128.0.0.0 192.168.179.9
Wed Apr 10 17:42:50 2024 ROUTE: CreateIpForwardEntry succeeded with dwForwardMetric1=35 and dwForwardType=4
Wed Apr 10 17:42:50 2024 Route addition via IPAPI succeeded [adaptive]
Wed Apr 10 17:42:50 2024 C:\Windows\system32\route.exe ADD 128.0.0.0 MASK 128.0.0.0 192.168.179.9
Wed Apr 10 17:42:50 2024 ROUTE: CreateIpForwardEntry succeeded with dwForwardMetric1=35 and dwForwardType=4
Wed Apr 10 17:42:50 2024 Route addition via IPAPI succeeded [adaptive]
Wed Apr 10 17:42:50 2024 MANAGEMENT: >STATE:1712742170,ADD_ROUTES,,,,,,
Wed Apr 10 17:42:50 2024 C:\Windows\system32\route.exe ADD 192.168.179.0 MASK 255.255.255.0 192.168.179.9
Wed Apr 10 17:42:50 2024 ROUTE: CreateIpForwardEntry succeeded with dwForwardMetric1=35 and dwForwardType=4
Wed Apr 10 17:42:50 2024 Route addition via IPAPI succeeded [adaptive]
Wed Apr 10 17:42:50 2024 Initialization Sequence Completed
Wed Apr 10 17:42:50 2024 MANAGEMENT: >STATE:1712742170,CONNECTED,SUCCESS,192.168.179.10,58.246.78.150,2094,172.168.2.122,50493

```



### 3. 回来路由不通

但是获取到的OpenVPN 地址是无法访问公网的，因为上联设备{交换机、路由器、防火墙没有配置回来的路由}，应在`相关设备配置192.168.179.0/24 -> 172.168.2.13的回向路由`

```
[root@sslvpn openvpn]# ping 39.156.66.10 -I tun0
PING 39.156.66.10 (39.156.66.10) from 192.168.179.1 tun0: 56(84) bytes of data.
^C
--- 39.156.66.10 ping statistics ---
48 packets transmitted, 0 received, 100% packet loss, time 47062ms


```





## Tengine

```bash
[root@nginx ~]# yum groupinstall -y "Development Tools" "Development and Creative Workstation"
[root@nginx ~]# cd /download/
[root@nginx download]# curl -OL http://tengine.taobao.org/download/tengine-2.3.2.tar.gz
[root@nginx download]# curl -OL http://ftp.pcre.org/pub/pcre/pcre-8.44.tar.gz
[root@nginx download]# curl -OL https://codeload.github.com/yaoweibin/ngx_http_substitutions_filter_module/zip/master
[root@nginx download]# ls
master  pcre-8.44.tar.gz  tengine-2.3.2.tar.gz
[root@nginx download]# groupadd -r -g 8080 tengine
[root@nginx download]# useradd -r -g 8080 -u 8080 -s /sbin/nologin -M tengine
[root@nginx download]# unzip master
[root@nginx download]# tar xf pcre-8.44.tar.gz -C /usr/local/
[root@nginx download]# tar xf tengine-2.3.2.tar.gz 
[root@nginx download]# ll
total 4944
-rw-r--r--  1 root root  120840 Mar 18 19:50 master
drwxr-xr-x  5 root root    4096 Aug  6  2019 ngx_http_substitutions_filter_module-master
-rw-r--r--  1 root root 2090750 Mar 18 19:50 pcre-8.44.tar.gz
drwxrwxr-x 13 root root    4096 Sep  5  2019 tengine-2.3.2
-rw-r--r--  1 root root 2835884 Mar 18 19:45 tengine-2.3.2.tar.gz
[root@nginx download]# cd tengine-2.3.2
[root@nginx tengine-2.3.2]# yum install -y openssl-devel
[root@nginx download]# ./configure --prefix=/usr/local/nginx --sbin-path=/usr/local/nginx/sbin/nginx --conf-path=/usr/local/nginx/conf/nginx.conf --error-log-path=/usr/local/nginx/logs/error.log --http-log-path=/usr/local/nginx/logs/access.log --pid-path=/usr/local/nginx/tengine.pid --user=tengine --group=tengine --with-pcre=/usr/local/pcre-8.44 --with-http_ssl_module --with-http_flv_module --with-http_stub_status_module --with-http_gzip_static_module --with-http_sub_module --with-stream --add-module=modules/ngx_http_upstream_session_sticky_module --add-module=/download/ngx_http_substitutions_filter_module-master --with-stream_ssl_module --add-module=modules/ngx_http_upstream_check_module --with-http_auth_request_module --with-http_gzip_static_module --with-http_random_index_module --with-http_sub_module
[root@nginx tengine-2.3.2]# make -j 4 && make install ;echo $?
[root@nginx conf]# cat /etc/init.d/tengine 
#!/bin/bash
#

# nginx - this script starts and stops the nginx daemon

#

# chkconfig: - 85 15

# description: Nginx is an HTTP(S) server, HTTP(S) reverse

# proxy and IMAP/POP3 proxy server

# processname: nginx

# config: /usr/local/nginx/conf/nginx.conf

# config: /etc/sysconfig/nginx

# pidfile: /var/run/nginx.pid

# Source function library.

. /etc/rc.d/init.d/functions

# Source networking configuration.

. /etc/sysconfig/network

# Check that networking is up.

[ "$NETWORKING" = "no" ] && exit 0

TENGINE_HOME="/usr/local/nginx/"
nginx=$TENGINE_HOME"sbin/nginx"
prog=$(basename $nginx)

NGINX_CONF_FILE=$TENGINE_HOME"conf/nginx.conf"

[ -f /etc/sysconfig/nginx ] && /etc/sysconfig/nginx

lockfile=/var/lock/subsys/nginx

start() {
    [ -x $nginx ] || exit 5
    [ -f $NGINX_CONF_FILE ] || exit 6
    echo -n $"Starting $prog: "
    daemon $nginx -c $NGINX_CONF_FILE
    retval=$?
    echo
    [ $retval -eq 0 ] && touch $lockfile
    return $retval
}

stop() {
    echo -n $"Stopping $prog: "
    killproc $prog -QUIT
    retval=$?
    echo
    [ $retval -eq 0 ] && rm -f $lockfile
    return $retval
    killall -9 nginx
}

restart() {
    configtest || return $?
    stop
    sleep 1
    start
}

reload() {
    configtest || return $?
    echo -n $"Reloading $prog: "
    killproc $nginx -HUP
    RETVAL=$?
    echo
}

force_reload() {
    restart
}

configtest() {
    $nginx -t -c $NGINX_CONF_FILE
}

rh_status() {
    status $prog
}

rh_status_q() {
    rh_status >/dev/null 2>&1
}

case "$1" in
start)
    rh_status_q && exit 0
    $1
;;
stop)
    rh_status_q || exit 0
    $1
;;
restart|configtest)
    $1
;;
reload)
    rh_status_q || exit 7
    $1
;;
force-reload)
    force_reload
;;
status)
    rh_status
;;
condrestart|try-restart)
    rh_status_q || exit 0
;;
*)

echo $"Usage: $0 {start|stop|status|restart|condrestart|try-restart|reload|force-reload|configtest}"
exit 2

esac
----------------------

[root@nginx tengine-2.3.2]# systemctl daemon-reload
[root@nginx tengine-2.3.2]# systemctl enable tengine
```



## Bind

```bash
[root@nginx conf]# sed -i '/^DNS/ s/^\(.*\)$/#\1/' /etc/sysconfig/network-scripts/ifcfg-eth0
[root@nginx conf]# yum install -y bind bind-libs bind-utils

[root@nginx conf]#  grep -Ev '#|^$|^/' /etc/named.conf
-----------------------

options {
	listen-on port 53 { 127.0.0.1; 10.10.10.240; };
	listen-on-v6 port 53 { ::1; };
	directory 	"/var/named";
	dump-file 	"/var/named/data/cache_dump.db";
	statistics-file "/var/named/data/named_stats.txt";
	memstatistics-file "/var/named/data/named_mem_stats.txt";
	recursing-file  "/var/named/data/named.recursing";
	secroots-file   "/var/named/data/named.secroots";
	allow-query     { localhost; any; };
	allow-transfer  { 10.10.10.240; };
	forward first;                    
    forwarders { 100.100.2.136; 100.100.2.138; };
	recursion yes;
	dnssec-enable yes;
	dnssec-validation yes;
	/* Path to ISC DLV key */
	bindkeys-file "/etc/named.root.key";
	managed-keys-directory "/var/named/dynamic";
	pid-file "/run/named/named.pid";
	session-keyfile "/run/named/session.key";
};
logging {
        channel default_debug {
                file "data/named.run";
                severity dynamic;
        };
};
zone "." IN {
	type hint;
	file "named.ca";
};
include "/etc/named.rfc1912.zones";

include "/etc/named.root.key";
-----------------------

[root@nginx conf]# named-checkconf
[root@nginx conf]# systemctl enable named.service
[root@nginx conf]# systemctl start named.service
[root@nginx conf]# sed -i '/^#DNS/ s/^#\(.*\)$/\1/' /etc/sysconfig/network-scripts/ifcfg-eth0
[root@nginx etc]# vim /etc/named.rfc1912.zones 
zone "hs.com" IN {
        type master;
        file "hs.com.zone";
};
zone "10.10.10.in-addr.arpa" IN {
        type master;
        file "10.10.10.zone";
};
[root@nginx named]# cat /var/named/hs.com.zone 
$TTL 3600
$ORIGIN hs.com.
@       IN      SOA     ns1.hs.com.   admin.hs.com. (
        2021031901
        1H ; refresh
        10M ; retry
        3D ; expire
        1D ; negative answer ttl
)
        		IN      NS      ns1
	     		IN      A       10.10.10.240
ns1     		IN      A       10.10.10.240
iptables.ops     	IN      A       10.10.10.250
nginx.ops     		IN      A       10.10.10.240
jumpserver.ops     	IN      A       10.10.10.230
docker01.ops     	IN      A       10.10.10.101
docker02.ops     	IN      A       10.10.10.102
;docker03.ops     	IN      A       10.10.10.103
iptables     		IN     CNAME    iptables.ops

[root@nginx named]# cat /var/named/10.10.10.zone 
$ORIGIN 10.10.10.in-addr.arpa.
@       3600	IN      SOA     hs.com.    admin.hs.com. (
        2021031901
        1H ; refresh
        10M ; retry
        3D ; expire
        1D ; negative answer ttl 
)
        		IN      NS      ns1.hs.com.
250       		IN      PTR     iptables.ops.hs.com.
240       		IN      PTR     nginx.ops.hs.com.
230       		IN      PTR     jumpserver.ops.hs.com.
101       		IN      PTR     docker01.ops.hs.com.
102       		IN      PTR     docker02.ops.hs.com.
;103       		IN      PTR     docker03.ops.hs.com.

[root@nginx named]# named-checkzone hs.com hs.com.zone 
zone hs.com/IN: loaded serial 2021031901
OK
[root@nginx named]# named-checkzone 10.10.10.in-addr.arpa 10.10.10.zone 
10.10.10.zone:9: using RFC1035 TTL semantics
zone 10.10.10.in-addr.arpa/IN: loaded serial 2021031901
OK
[root@nginx named]# rndc reload
server reload successful


# 配置CentOS7使用自建DNS服务器，自己想的方式，后面有其它配置DNS服务器方式
[root@nginx named]# sudo cat /etc/rc.d/rc.local
# auto set custom DNS Server.
echo 'nameserver 10.10.10.240' > /etc/resolv.conf
echo 'nameserver 100.100.2.136' >> /etc/resolv.conf
```



## Jumpserver

```bash
require: mysql>=5.7,redis>=5.0
mysql: jumpserver
redis: kkD

mysql> create database jumpserver default charset 'utf8' collate 'utf8_bin';
mysql> grant all on jumpserver.* to jumpserver_admin@'127.0.0.1' identified by '123';
mysql> flush privileges;

[root@jumpserver /srv/salt/base/init]# cd /download/
[root@jumpserver /download]# axel -n 30 https://github.com/jumpserver/jumpserver/releases/download/v2.5.4/jumpserver-v2.5.4.tar.gz
[root@jumpserver /download]# tar xf jumpserver-v2.5.4.tar.gz -C /opt/
[root@jumpserver /download]# ln -sv /opt/jumpserver-v2.5.4/ /opt/jumpserver
[root@jumpserver /opt]# python3.6 -m venv /opt/py3
[root@jumpserver /opt]# source /opt/py3/bin/activate
(py3) [root@jumpserver /opt/jumpserver/requirements]#
(py3) [root@jumpserver /opt/jumpserver/requirements]# yum install -y `cat rpm_requirements.txt`
(py3) [root@jumpserver /opt/jumpserver/requirements]# yum install -y python3-devel
(py3) [root@jumpserver /opt/jumpserver/requirements]# pip install -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple/
(py3) [root@jumpserver /opt/jumpserver/requirements]# cd /opt/jumpserver
(py3) [root@jumpserver /opt/jumpserver]# cp config_example.yml config.yml 
(py3) [root@jumpserver /opt/jumpserver]# cat /dev/urandom | tr -dc 'a-zA-Z0-9' | head -c 49;echo ''
mBsC2jwcfctRQ4VFTWeti
(py3) [root@jumpserver /opt/jumpserver]# cat /dev/urandom | tr -dc 'a-zA-Z0-9' | head -c 15;echo ''
UlxeRfB4PenMzXK

(py3) [root@jumpserver /opt/jumpserver]# grep -Ev '^$|#' /opt/jumpserver/config.yml    --key和token都必须一样
----

SECRET_KEY: SK
BOOTSTRAP_TOKEN: UlxeRfB4P
DEBUG: true
LOG_LEVEL: DEBUG
LOG_DIR: /var/log/jumpserver.log
SESSION_COOKIE_AGE: 86400
SESSION_EXPIRE_AT_BROWSER_CLOSE: true
DB_ENGINE: mysql
DB_HOST: 127.0.0.1
DB_PORT: 3306
DB_USER: jumpserver_admin
DB_PASSWORD: password
DB_NAME: jumpserver
HTTP_BIND_HOST: 0.0.0.0
HTTP_LISTEN_PORT: 8080
WS_LISTEN_PORT: 8070
REDIS_HOST: 127.0.0.1
REDIS_PORT: 6379
REDIS_PASSWORD: password

WINDOWS_SKIP_ALL_MANUAL_PASSWORD: true
----

(py3) [root@jumpserver /opt/jumpserver]# /opt/jumpserver/jms start -d
(py3) [root@jumpserver /opt/jumpserver]# netstat -tnlp
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name    
tcp        0      0 0.0.0.0:8070            0.0.0.0:*               LISTEN      18664/python3.6     
tcp        0      0 0.0.0.0:3306            0.0.0.0:*               LISTEN      13633/mysqld        
tcp        0      0 0.0.0.0:6379            0.0.0.0:*               LISTEN      15955/redis-server  
tcp        0      0 0.0.0.0:8080            0.0.0.0:*               LISTEN      18644/python3.6     
tcp        0      0 0.0.0.0:5555            0.0.0.0:*               LISTEN      18660/python3.6     
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      1008/sshd           
tcp        0      0 0.0.0.0:4505            0.0.0.0:*               LISTEN      6136/python3        
tcp        0      0 0.0.0.0:4506            0.0.0.0:*               LISTEN      6164/python3        
tcp6       0      0 :::9100                 :::*                    LISTEN      23273/node_exporter 
tcp6       0      0 :::5555                 :::*                    LISTEN      18660/python3.6     
--koko部署
(py3) [root@jumpserver /download]# axel -n 30 https://github.com/jumpserver/koko/releases/download/v2.5.4/koko-v2.5.4-linux-amd64.tar.gz
(py3) [root@jumpserver /download]# axel -n 30 https://download.jumpserver.org/public/kubectl.tar.gz
(py3) [root@jumpserver /download]# tar xf koko-v2.5.4-linux-amd64.tar.gz -C /opt/
(py3) [root@jumpserver /download]# tar xf kubectl.tar.gz 
(py3) [root@jumpserver /download]# cd /opt/
(py3) [root@jumpserver /opt]# ln -sv koko-v2.5.4-linux-amd64/ koko
(py3) [root@jumpserver /opt]# cd koko
(py3) [root@jumpserver /opt/koko]# mv kubectl /usr/local/bin/
(py3) [root@jumpserver /download]# chmod 755 /download/kubectl
(py3) [root@jumpserver /opt/koko]# mv /download/kubectl /usr/local/bin/rawkubectl
(py3) [root@jumpserver /opt/koko]# rm -rf /download/kubectl.tar.gz
(py3) [root@jumpserver /opt/koko]# cp config_example.yml config.yml 
(py3) [root@jumpserver /opt/koko]# vim config.yml
(py3) [root@jumpserver /opt/koko]# grep -Ev '#|^$' config.yml
CORE_HOST: http://127.0.0.1:8080
BOOTSTRAP_TOKEN: UlxeRfB4P K
REDIS_HOST: 127.0.0.1
REDIS_PORT: 6379
REDIS_PASSWORD: password 
REDIS_DB_ROOM: 6
(py3) [root@jumpserver /opt/koko]# /opt/koko/koko -s start -d 
(py3) [root@jumpserver /opt/koko]# netstat -tnlp | grep -E '2222|5000'
tcp6       0      0 :::5000                 :::*                    LISTEN      26822/koko          
tcp6       0      0 :::2222                 :::*                    LISTEN      26822/koko          
--Docker 部署 Guacamole 组件
docker run --name jms_guacamole -d \
  --restart=always \
  -p 127.0.0.1:8081:8080 \
  -e JUMPSERVER_KEY_DIR=/config/guacamole/key \
  -e JUMPSERVER_SERVER=http://10.10.10.230:8080 \
  -e BOOTSTRAP_TOKEN=Ulxe \
  -e GUACAMOLE_LOG_LEVEL=ERROR \
  jumpserver/guacamole:v2.5.4
--配置Lina和luna 组件
(py3) [root@jumpserver /download]# axel -n 30 https://github.com/jumpserver/lina/releases/download/v2.5.4/lina-v2.5.4.tar.gz
(py3) [root@jumpserver /download]# axel -n 30 https://github.com/jumpserver/luna/releases/download/v2.5.4/luna-v2.5.4.tar.gz
(py3) [root@jumpserver /download]# tar xf lina-v2.5.4.tar.gz -C /opt
(py3) [root@jumpserver /download]# tar xf luna-v2.5.4.tar.gz -C /opt
(py3) [root@jumpserver /download]# ln -sv /opt/lina-v2.5.4 /opt/lina && chown -R tengine:tengine /opt/lina-v2.5.4
(py3) [root@jumpserver /download]# ln -sv /opt/luna-v2.5.4 /opt/luna && chown -R tengine:tengine /opt/luna-v2.5.4
--警告迁移
(py3) [root@jumpserver /opt/koko]# cd /opt/jumpserver/apps/
(py3) [root@jumpserver /opt/jumpserver/apps]# ./manage.py makemigrations
Migrations for 'assets':
  assets/migrations/0063_auto_20210324_1925.py
    - Change Meta options on node
(py3) [root@jumpserver /opt/jumpserver/apps]# ./manage.py migrate
Operations to perform:
  Apply all migrations: admin, applications, assets, audits, auth, authentication, captcha, common, contenttypes, django_cas_ng, django_celery_beat, jms_oidc_rp, ops, orgs, perms, sessions, settings, terminal, tickets, users
Running migrations:
  Applying assets.0063_auto_20210324_1925... OK

注：koko组件相关连接mysql，当报mysql协议的客户端未安装时，因为jumpserver本机上未找到mysql，当是二进制安装时，
jumpserver不会识别，此时需要yum安装mariadb才行，yum install mariadb mariadb-libs mariadb-devel即可解决。
[root@jumpserver ~]# mv /etc/my.cnf.rpmsave /etc/my.cnf


--部署tengine
[root@jumpserver /opt]# salt 'jumpserver*' state.sls tengine.service saltenv=prod

[root@jumpserver /usr/local/nginx/conf]# grep -Ev '#|^$' nginx.conf
---

worker_processes  4;
events {
    worker_connections  10240;
}
http {
	include       mime.types;
	default_type  application/octet-stream;
	log_format  main  '$remote_addr - $remote_user [$time_local] "$request"'
                               '$status $body_bytes_sent "$http_referer"'
                               '"$http_user_agent" "$http_x_forwarded_for"';
	log_format log_json '{ "@timestamp": "$time_local", '
        '"remote_addr": "$remote_addr", '
        '"referer": "$http_referer", '
        '"host": "$host", '
        '"request": "$request", '
        '"status": $status, '
        '"bytes": $body_bytes_sent, '
        '"agent": "$http_user_agent", '
        '"x_forwarded": "$http_x_forwarded_for", '
        '"up_addr": "$upstream_addr",'
        '"up_host": "$upstream_http_host",'
        '"up_resp_time": "$upstream_response_time",'
        '"request_time": "$request_time"'
        ' }';
        access_log  logs/access.log  log_json;
	client_max_body_size 200m;
	underscores_in_headers on;
	server_tokens off;
	sendfile        on;
	keepalive_timeout  1024;
	gzip on;
	gzip_proxied any;
	gzip_http_version 1.1;
	gzip_min_length 1100;
	gzip_comp_level 5;
	gzip_buffers 8 16k;
	gzip_types application/json text/json text/plain text/xml text/css application/x-javascript application/xml application/xml+rss text/javascript application/atom+xml image/gif image/jpeg image/png;
	gzip_vary on;
    server {
    	listen 80;
	    location /ui/ {
	        try_files $uri / /index.html;
	        alias /opt/lina/;
	    }
	

	    location /luna/ {
	        try_files $uri / /index.html;
	    }
	
	    location /media/ {
	        add_header Content-Encoding gzip;
	    }
	
	    location /static/ {
	    }
	
	    location /koko/ {
	        proxy_pass       http://localhost:5000;
	        proxy_buffering off;
	        proxy_http_version 1.1;
	        proxy_set_header Upgrade $http_upgrade;
	        proxy_set_header Connection "upgrade";
	        proxy_set_header X-Real-IP $remote_addr;
	        proxy_set_header Host $host;
	        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
	        access_log off;
	    }
	
	    location /guacamole/ {
	        proxy_pass       http://localhost:8081/;
	        proxy_buffering off;
	        proxy_http_version 1.1;
	        proxy_set_header Upgrade $http_upgrade;
	        proxy_set_header Connection $http_connection;
	        proxy_set_header X-Real-IP $remote_addr;
	        proxy_set_header Host $host;
	        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
	        access_log off;
	    }
	
	    location /ws/ {
	        proxy_set_header X-Real-IP $remote_addr;
	        proxy_set_header Host $host;
	        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
	        proxy_pass http://localhost:8070;
	        proxy_http_version 1.1;
	        proxy_buffering off;
	        proxy_set_header Upgrade $http_upgrade;
	        proxy_set_header Connection "upgrade";
	    }
	
	    location /api/ {
	        proxy_pass http://localhost:8080;
	        proxy_set_header X-Real-IP $remote_addr;
	        proxy_set_header Host $host;
	        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
	    }
	
	    location /core/ {
	        proxy_pass http://localhost:8080;
	        proxy_set_header X-Real-IP $remote_addr;
	        proxy_set_header Host $host;
	        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
	    }
	
	    location / {
	        rewrite ^/(.*)$ /ui/$1 last;
	    }
	}	
	server {
		listen       8088;
		location / {
	                    add_header backendIP $upstream_addr;
	                    proxy_redirect off;
	                    proxy_set_header Host $host;
	                    proxy_read_timeout 300s;
	                    proxy_buffer_size  128k;
	                    proxy_buffers   32 32k;
	                    proxy_busy_buffers_size 128k; 
	                    proxy_set_header X-Real-IP $remote_addr;
	                    proxy_set_header X-Real-Port $remote_port;
	                    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
	            }
		error_page   500 502 503 504  /50x.html;
		location = /50x.html {
			root   html;
		}
		location /NginxStatus {
			stub_status on;
		}
	}

}
---

[root@jumpserver /usr/local/nginx/conf]# cat /etc/init.d/tengine   --注：脚本出错在于注释过多，删除lock,pid等注释即可。
---

#!/bin/bash
#

# nginx - this script starts and stops the nginx daemon

#

# chkconfig: - 85 15

# description: Nginx is an HTTP(S) server, HTTP(S) reverse

# proxy and IMAP/POP3 proxy server

# processname: nginx

# Source function library.

. /etc/rc.d/init.d/functions

# Source networking configuration.

. /etc/sysconfig/network

# Check that networking is up.

[ "$NETWORKING" = "no" ] && exit 0

TENGINE_HOME="/usr/local/nginx/"
nginx=$TENGINE_HOME"sbin/nginx"
prog=$(basename $nginx)

NGINX_CONF_FILE=$TENGINE_HOME"conf/nginx.conf"

[ -f /etc/sysconfig/nginx ] && /etc/sysconfig/nginx

lockfile=/var/lock/subsys/nginx

start() {
    [ -x $nginx ] || exit 5
    [ -f $NGINX_CONF_FILE ] || exit 6
    echo -n $"Starting $prog: "
    daemon $nginx -c $NGINX_CONF_FILE
    retval=$?
    echo
    [ $retval -eq 0 ] && touch $lockfile
    return $retval
}

stop() {
    echo -n $"Stopping $prog: "
    killproc $prog -QUIT
    retval=$?
    echo
    [ $retval -eq 0 ] && rm -f $lockfile
    return $retval
    killall -9 nginx
}

restart() {
    configtest || return $?
    stop
    sleep 1
    start
}

reload() {
    configtest || return $?
    echo -n $"Reloading $prog: "
    killproc $nginx -HUP
    RETVAL=$?
    echo
}

force_reload() {
    restart
}

configtest() {
    $nginx -t -c $NGINX_CONF_FILE
}

rh_status() {
    status $prog
}

rh_status_q() {
    rh_status >/dev/null 2>&1
}

case "$1" in
start)
    rh_status_q && exit 0
    $1
;;
stop)
    rh_status_q || exit 0
    $1
;;
restart|configtest)
    $1
;;
reload)
    rh_status_q || exit 7
    $1
;;
force-reload)
    force_reload
;;
status)
    rh_status
;;
condrestart|try-restart)
    rh_status_q || exit 0
;;
*)

echo $"Usage: $0 {start|stop|status|restart|condrestart|try-restart|reload|force-reload|configtest}"
exit 2
esac
---jumpserver boot shell
[root@jumpserver ~]# cat /etc/init.d/jumpserver 
#!/bin/sh

# chkconfig: 35 90 10


# nodebook: manual kill beat

# ps aux | grep 'celery beat' | grep -v grep | awk '{print $2}' | xargs -I {} kill -9 {}

#start jumpserver
start(){
	source /opt/py3/bin/activate
	cd /opt/jumpserver && ./jms start -d
	cd /opt/koko && ./koko -s start -d 
	#RESULT=`docker ps | grep jms_guacamole >& /dev/null && echo 0 || echo 1`
	#if [ ${RESULT} == 1 ];then
	#	docker start jms_guacamole
	#fi
	docker restart jms_guacamole
	[ $? == 0 ] && echo "jumpserver start sucessful" || echo "jumpserver start failure"
}

stop(){
	source /opt/py3/bin/activate
        cd /opt/koko && ./koko -s stop
        cd /opt/jumpserver && ./jms stop
	RESULT=`docker ps | grep jms_guacamole >& /dev/null && echo 0 || echo 1`
	if [ ${RESULT} == 0 ];then
		docker stop jms_guacamole
	fi
	[ $? == 0 ] && echo "jumpserver stop sucessful" || echo "jumpserver stop failure"
	ps aux | grep 'celery beat' | grep -v grep | awk '{print $2}' | xargs -I {} kill -9 {}
}

status(){
	NET_RESULT=`netstat -tnlp | egrep "8080|8070|5000|2222|5555|8081" | wc -l`
	if [[ "${NET_RESULT}" < 6 ]];then
		echo "jumpserver is stop"
	else
        	echo "jumpserver is running"
        fi
}

case "$1" in 
	start)
		start
	;;
	stop)
		stop
	;;
	status)
		status
	;;
	*)
		echo "Usage: $0 [ start | stop | status ]"
	;;
esac
```



## Iptables

```bash
# 配置规则
iptables -t nat -R PREROUTING 1 -d 47.100.73.115 -p tcp --dport 80 -j DNAT --to-destination 10.10.10.240:80 
iptables -t nat -R PREROUTING 2 -d 47.100.73.115 -p tcp --dport 443 -j DNAT --to-destination 10.10.10.240:443
#iptables -t nat -R PREROUTING 1 ! -s 10.0.0.0/8 -d 47.100.73.115 -p tcp --dport 80 -j DNAT --to-destination 10.10.10.240:80 
#iptables -t nat -R PREROUTING 2 ! -s 10.0.0.0/8 -d 47.100.73.115 -p tcp --dport 443 -j DNAT --to-destination 10.10.10.240:443
#iptables -t nat -I POSTROUTING 1 ! -s 10.0.0.0/8 -d 10.10.10.240 -p tcp --dport 80 -j SNAT --to-source 10.10.10.250

#iptables -t nat -I POSTROUTING 2 ! -s 10.0.0.0/8 -d 10.10.10.240 -p tcp --dport 443 -j SNAT --to-source 10.10.10.250
--------------

[root@iptables ~]# cat /etc/sysconfig/iptables

# Generated by iptables-save v1.4.21 on Thu Mar 25 08:59:19 2021

*nat
:PREROUTING ACCEPT [701:41429]
:INPUT ACCEPT [37:2340]
:OUTPUT ACCEPT [76:4963]
:POSTROUTING ACCEPT [118:7235]
-A PREROUTING -d 47.100.73.115/32 -p tcp -m tcp --dport 80 -j DNAT --to-destination 10.10.10.240:80
-A PREROUTING -d 47.100.73.115/32 -p tcp -m tcp --dport 443 -j DNAT --to-destination 10.10.10.240:443
-A POSTROUTING -s 10.0.0.0/8 -o eth1 -j SNAT --to-source 47.100.73.115
-A POSTROUTING -s 192.168.177.0/24 -j MASQUERADE
COMMIT

# Completed on Thu Mar 25 08:59:19 2021

# Generated by iptables-save v1.4.21 on Thu Mar 25 08:59:19 2021

*filter
:INPUT DROP [330:133270]
:FORWARD ACCEPT [9432993:6241960630]
:OUTPUT ACCEPT [6309905:4980444813]
-A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT
-A INPUT -p icmp -j ACCEPT
-A INPUT -i lo -j ACCEPT
-A INPUT -p tcp -m tcp --dport 1194 -j ACCEPT
-A INPUT -s 192.168.177.0/24 -p tcp -m tcp --dport 9100 -j ACCEPT
-A INPUT -s 192.168.177.0/24 -p tcp -m tcp --dport 9572 -j ACCEPT
-A INPUT -s 10.0.0.0/8 -p tcp -m tcp --dport 9572 -j ACCEPT
-A INPUT -s 222.66.21.210/32 -p tcp -m tcp --dport 9572 -j ACCEPT
-A INPUT -p tcp -m tcp --dport 9572 -j DROP
COMMIT

# Completed on Thu Mar 25 08:59:19 2021
```





## Elasticsearch和Kibana



### 1. 配置数据盘

#### 1.1 创建LVM分区

```bash
[opsuser@elasticsearch001 ~]$ sudo fdisk -l

Disk /dev/vda: 42.9 GB, 42949672960 bytes, 83886080 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk label type: dos
Disk identifier: 0x000bdc9e

   Device Boot      Start         End      Blocks   Id  System
/dev/vda1   *        2048    83886046    41941999+  83  Linux

Disk /dev/vdb: 536.9 GB, 536870912000 bytes, 1048576000 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes



[opsuser@elasticsearch001 ~]$ sudo fdisk /dev/vdb
Command (m for help): n

Partition type:
   p   primary (0 primary, 0 extended, 4 free)
   e   extended
Select (default p): p
Partition number (1-4, default 1):
First sector (2048-1048575999, default 2048):
Using default value 2048
Last sector, +sectors or +size{K,M,G} (2048-1048575999, default 1048575999):
Using default value 1048575999
Partition 1 of type Linux and of size 500 GiB is set


Command (m for help): p

Disk /dev/vdb: 536.9 GB, 536870912000 bytes, 1048576000 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk label type: dos
Disk identifier: 0x42fcba93

   Device Boot      Start         End      Blocks   Id  System
/dev/vdb1            2048  1048575999   524286976   83  Linux

Command (m for help): t
Selected partition 1
Hex code (type L to list all codes): 8e
Changed type of partition 'Linux' to 'Linux LVM'

Command (m for help): p

Disk /dev/vdb: 536.9 GB, 536870912000 bytes, 1048576000 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk label type: dos
Disk identifier: 0x42fcba93

   Device Boot      Start         End      Blocks   Id  System
/dev/vdb1            2048  1048575999   524286976   8e  Linux LVM
Command (m for help): w
The partition table has been altered!

Calling ioctl() to re-read partition table.
Syncing disks.
```



#### 1.2 创建逻辑分区

```bash
# 安装LVM命令行工具
[opsuser@elasticsearch001 ~]$ sudo yum install lvm2 -y

# 查看LVM分区名
[opsuser@elasticsearch001 ~]$ sudo fdisk -l

Disk /dev/vda: 42.9 GB, 42949672960 bytes, 83886080 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk label type: dos
Disk identifier: 0x000bdc9e

   Device Boot      Start         End      Blocks   Id  System
/dev/vda1   *        2048    83886046    41941999+  83  Linux

Disk /dev/vdb: 536.9 GB, 536870912000 bytes, 1048576000 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk label type: dos
Disk identifier: 0x42fcba93

   Device Boot      Start         End      Blocks   Id  System
/dev/vdb1            2048  1048575999   524286976   8e  Linux LVM

# 创建PV
[opsuser@elasticsearch001 ~]$ sudo pvcreate /dev/vdb1
  Physical volume "/dev/vdb1" successfully created.
[opsuser@elasticsearch001 ~]$ sudo pvdisplay
  "/dev/vdb1" is a new physical volume of "<500.00 GiB"
  --- NEW Physical volume ---
  PV Name               /dev/vdb1
  VG Name
  PV Size               <500.00 GiB
  Allocatable           NO
  PE Size               0
  Total PE              0
  Free PE               0
  Allocated PE          0
  PV UUID               hboH8y-r9bS-kEqR-NByv-scS7-56lN-MWRIf7

# 创建VG
[opsuser@elasticsearch001 ~]$ sudo vgcreate data /dev/vdb1
  Volume group "data" successfully created
[opsuser@elasticsearch001 ~]$ sudo vgdisplay
  --- Volume group ---
  VG Name               data
  System ID
  Format                lvm2
  Metadata Areas        1
  Metadata Sequence No  1
  VG Access             read/write
  VG Status             resizable
  MAX LV                0
  Cur LV                0
  Open LV               0
  Max PV                0
  Cur PV                1
  Act PV                1
  VG Size               <500.00 GiB
  PE Size               4.00 MiB
  Total PE              127999
  Alloc PE / Size       0 / 0
  Free  PE / Size       127999 / <500.00 GiB
  VG UUID               EbODts-bhJv-pRzw-Lzty-85Tl-AOzP-QY67ev
  
# 创建LV
[opsuser@elasticsearch001 ~]$ sudo lvcreate -l 100%FREE -n elasticsearch data
  Logical volume "elasticsearch" created.
[opsuser@elasticsearch001 ~]$ sudo lvdisplay
  --- Logical volume ---
  LV Path                /dev/data/elasticsearch
  LV Name                elasticsearch
  VG Name                data
  LV UUID                B5MbpV-zGpu-3dzZ-71iK-8OQt-lbdI-jcQROm
  LV Write Access        read/write
  LV Creation host, time elasticsearch001.ops.hs.com, 2024-03-08 11:56:04 +0800
  LV Status              available
  # open                 0
  LV Size                <500.00 GiB
  Current LE             127999
  Segments               1
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     8192
  Block device           252:0

# 格式化并挂载LVM卷
[opsuser@elasticsearch001 ~]$ sudo mkfs -t xfs /dev/mapper/data-elasticsearch
meta-data=/dev/mapper/data-elasticsearch isize=512    agcount=4, agsize=32767744 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=1        finobt=0, sparse=0
data     =                       bsize=4096   blocks=131070976, imaxpct=25
         =                       sunit=0      swidth=0 blks
naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
log      =internal log           bsize=4096   blocks=63999, version=2
         =                       sectsz=512   sunit=0 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0
[opsuser@elasticsearch001 ~]$ sudo mkdir -p /data
[opsuser@elasticsearch001 ~]$ sudo mount /dev/mapper/data-elasticsearch /data
[opsuser@elasticsearch001 ~]$ sudo df -TH
Filesystem                     Type      Size  Used Avail Use% Mounted on
devtmpfs                       devtmpfs   17G     0   17G   0% /dev
tmpfs                          tmpfs      17G     0   17G   0% /dev/shm
tmpfs                          tmpfs      17G  517k   17G   1% /run
tmpfs                          tmpfs      17G     0   17G   0% /sys/fs/cgroup
/dev/vda1                      ext4       43G  4.1G   37G  11% /
tmpfs                          tmpfs     3.4G     0  3.4G   0% /run/user/1000
/dev/mapper/data-elasticsearch xfs       537G   34M  537G   1% /data

# 配置开机自动挂载
[opsuser@elasticsearch001 ~]$ cat /etc/fstab

#
# /etc/fstab
# Created by anaconda on Thu Jan 25 02:21:47 2024
#
# Accessible filesystems, by reference, are maintained under '/dev/disk'
# See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info
#
UUID=05c4558d-b072-4a70-a121-47e00a327778 /                       ext4    defaults        1 1
/dev/mapper/data-elasticsearch  /data   xfs     defaults 1 1
[opsuser@elasticsearch001 ~]$ sudo umount /data
# 自动读取/etc/fstab配置并自动挂载，检查/etc/fstab配置是否正常
[opsuser@elasticsearch001 ~]$ sudo mount -a
[opsuser@elasticsearch001 ~]$ sudo df -TH
Filesystem                     Type      Size  Used Avail Use% Mounted on
devtmpfs                       devtmpfs   17G     0   17G   0% /dev
tmpfs                          tmpfs      17G     0   17G   0% /dev/shm
tmpfs                          tmpfs      17G  517k   17G   1% /run
tmpfs                          tmpfs      17G     0   17G   0% /sys/fs/cgroup
/dev/vda1                      ext4       43G  4.1G   37G  11% /
tmpfs                          tmpfs     3.4G     0  3.4G   0% /run/user/1000
/dev/mapper/data-elasticsearch xfs       537G   34M  537G   1% /data

```





### 2. 单节点部署Elasticsearch

#### 2.1 下载

```bash
# 下载Elasticsearch和Kibana
[opsuser@elasticsearch001 ~]$ sudo mkdir -p /download
[opsuser@elasticsearch001 ~]$ cd /download
[opsuser@elasticsearch001 /download]$ sudo curl -OL https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.7.1-linux-x86_64.tar.gz
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  299M  100  299M    0     0  7988k      0  0:00:38  0:00:38 --:--:-- 6797k
[opsuser@elasticsearch001 /download]$ sudo curl -OL https://artifacts.elastic.co/downloads/kibana/kibana-7.7.1-linux-x86_64.tar.gz
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  266M  100  266M    0     0  8804k      0  0:00:30  0:00:30 --:--:-- 10.2M
[opsuser@elasticsearch001 /download]$ ll
total 579508
-rw-r--r-- 1 root root 314445622 Mar  8 11:30 elasticsearch-7.7.1-linux-x86_64.tar.gz
-rw-r--r-- 1 root root 278963930 Mar  8 11:31 kibana-7.7.1-linux-x86_64.tar.gz
```





#### 2.2 Elasticsearch

##### 2.2.1 安装服务

```bash
# 优化系统参数
[opsuser@elasticsearch001 /usr/local/elasticsearch]$ cat /etc/security/limits.d/99-ansible.conf
*             soft    core            unlimited
*             hard    core            unlimited
*             soft    nproc           1000000
*             hard    nproc           1000000
root          soft    nproc           unlimited
root          hard    nproc           unlimited
# nofile最大值为 1048576(2**20)
*             soft    nofile          1000000
*             hard    nofile          1000000
root          soft    nofile          1000000
root          hard    nofile          1000000
*             soft    memlock         unlimited
*             hard    memlock         unlimited
*             soft    msgqueue        8192000
*             hard    msgqueue        8192000

# 优化内核参数
[opsuser@elasticsearch001 /usr/local/elasticsearch]$ cat /etc/sysctl.d/99-sysctl.conf
vm.swappiness=0
kernel.sysrq=1

net.ipv4.neigh.default.gc_stale_time=120

# see details in https://help.aliyun.com/knowledge_detail/39428.html
net.ipv4.conf.all.rp_filter=0
net.ipv4.conf.default.rp_filter=0
net.ipv4.conf.default.arp_announce=2
net.ipv4.conf.lo.arp_announce=2
net.ipv4.conf.all.arp_announce=2

# see details in https://help.aliyun.com/knowledge_detail/41334.html
net.ipv4.tcp_max_tw_buckets=5000
net.ipv4.tcp_syncookies=1
net.ipv4.tcp_max_syn_backlog=8192
net.ipv4.tcp_synack_retries=2
net.ipv4.tcp_slow_start_after_idle=0

net.ipv4.ip_local_port_range=10001 65000
net.ipv4.ip_forward=1
net.ipv4.tcp_tw_reuse=1
net.ipv4.tcp_tw_recycle=0
net.ipv4.tcp_keepalive_time=1200
net.ipv6.conf.all.disable_ipv6=1
vm.overcommit_memory=1
vm.panic_on_oom=0
fs.inotify.max_user_instances=8192
fs.inotify.max_user_watches=1048576
fs.file-max=52706963
fs.nr_open=52706963
vm.max_map_count=262144

# 解压
[opsuser@elasticsearch001 /download]$ sudo tar xf elasticsearch-7.7.1-linux-x86_64.tar.gz -C /usr/local/
[opsuser@elasticsearch001 /download]$ cd /usr/local/
[opsuser@elasticsearch001 /usr/local]$ sudo ln -sv elasticsearch-7.7.1/ elasticsearch
[opsuser@elasticsearch001 /usr/local]$ cd /usr/local/elasticsearch
[opsuser@elasticsearch001 /usr/local/elasticsearch]$ ll
total 576
drwxr-xr-x  2 root root   4096 May 29  2020 bin
drwxr-xr-x  3 root root   4096 Mar  8 11:39 config
drwxr-xr-x  9 root root   4096 May 29  2020 jdk
drwxr-xr-x  3 root root   4096 May 29  2020 lib
-rw-r--r--  1 root root  13675 May 29  2020 LICENSE.txt
drwxr-xr-x  2 root root   4096 May 29  2020 logs
drwxr-xr-x 45 root root   4096 May 29  2020 modules
-rw-r--r--  1 root root 534664 May 29  2020 NOTICE.txt
drwxr-xr-x  2 root root   4096 May 29  2020 plugins
-rw-r--r--  1 root root   8165 May 29  2020 README.asciidoc
```



##### 2.2.2 配置服务

```bash
# 配置elasticsearch
[opsuser@elasticsearch001 /usr/local/elasticsearch]$ sudo vim config/elasticsearch.yml
cluster.name: hotelES
node.name: hotel01
path.data: /data/elasticsearch7/data
path.logs: /data/elasticsearch7/log
path.repo: /data/elasticsearch7/backups
network.host: 0.0.0.0
http.port: 9200
transport.tcp.port: 9300
xpack.security.enabled: true # 这条配置表示开启xpack认证机制
xpack.security.transport.ssl.enabled: true  #这条如果不配，es将起不来
cluster.initial_master_nodes: ["10.10.10.201"]

# 创建系统用户及配置相关权限
[opsuser@elasticsearch001 /usr/local/elasticsearch]$ sudo groupadd -r elasticsearch
[opsuser@elasticsearch001 /usr/local/elasticsearch]$ sudo useradd -r -M -s /sbin/nologin -g elasticsearch elasticsearch
[opsuser@elasticsearch001 /usr/local/elasticsearch]$ sudo chown -R elasticsearch.elasticsearch /usr/local/elasticsearch-7.7.1/
[opsuser@elasticsearch001 /usr/local/elasticsearch]$ sudo mkdir /data/elasticsearch7
[opsuser@elasticsearch001 /usr/local/elasticsearch]$ sudo chown -R elasticsearch.elasticsearch /data/elasticsearch7/
```



##### 2.2.3 启动服务

```bash
# 配置elasticsearch服务
[opsuser@elasticsearch001 /usr/local/elasticsearch]$ sudo cat /usr/lib/systemd/system/elasticsearch.service
[Unit]
Description=https://elastic.co
After=network-online.target

[Service]
User=elasticsearch
Group=elasticsearch
Type=simple
ExecStart=/usr/local/elasticsearch/bin/elasticsearch
Restart=on-failure
LimitNOFILE=1000000

[Install]
WantedBy=multi-user.target

# 启动服务
[opsuser@elasticsearch001 /usr/local/elasticsearch]$ sudo systemctl daemon-reload
[opsuser@elasticsearch001 /usr/local/elasticsearch]$ sudo systemctl start elasticsearch.service
[opsuser@elasticsearch001 /usr/local/elasticsearch]$ sudo systemctl status elasticsearch.service
[opsuser@elasticsearch001 /usr/local/kibana]$ sudo systemctl enable elasticsearch.service

# 调整elasticsearch的内存为10G
[opsuser@elasticsearch001 /usr/local/elasticsearch]$ sudo grep '\-Xm' config/jvm.options
## -Xms4g
## -Xmx4g
-Xms16g
-Xmx16g

# 重启服务
[opsuser@elasticsearch001 /usr/local/elasticsearch]$ sudo systemctl restart elasticsearch.service

# 查看内存配置是否生效
[opsuser@elasticsearch001 /usr/local/elasticsearch]$ sudo ps -ef | grep elasticsearch | grep '\-Xm'
elastic+  2739     1 45 13:50 ?        00:00:24 /usr/local/elasticsearch/jdk/bin/java -Xshare:auto -Des.networkaddress.cache.ttl=60 -Des.networkaddress.cache.negative.ttl=10 -XX:+AlwaysPreTouch -Xss1m -Djava.awt.headless=true -Dfile.encoding=UTF-8 -Djna.nosys=true -XX:-OmitStackTraceInFastThrow -XX:+ShowCodeDetailsInExceptionMessages -Dio.netty.noUnsafe=true -Dio.netty.noKeySetOptimization=true -Dio.netty.recycler.maxCapacityPerThread=0 -Dio.netty.allocator.numDirectArenas=0 -Dlog4j.shutdownHookEnabled=false -Dlog4j2.disable.jmx=true -Djava.locale.providers=SPI,COMPAT -Xms10g -Xmx10g -XX:+UseG1GC -XX:G1ReservePercent=25 -XX:InitiatingHeapOccupancyPercent=30 -Djava.io.tmpdir=/tmp/elasticsearch-2004827100829949371 -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=data -XX:ErrorFile=logs/hs_err_pid%p.log -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m -XX:MaxDirectMemorySize=5368709120 -Des.path.home=/usr/local/elasticsearch -Des.path.conf=/usr/local/elasticsearch/config -Des.distribution.flavor=default -Des.distribution.type=tar -Des.bundled_jdk=true -cp /usr/local/elasticsearch/lib/* org.elasticsearch.bootstrap.Elasticsearch
```





##### 2.2.4 配置账号

ES中内置了几个管理其他集成组件的账号，在使用之前，需要先添加一下密码：

* elastic
* apm_system
* kibana
* logstash_system
* beats_system
* remote_monitoring_user

**只有elastic用户才能登录kibana，而且elastic用户是admin权限**



**为集成组件账号设置密码**

```bash
# 手动配置密码-方式一
[opsuser@elasticsearch001 /usr/local/elasticsearch]$ sudo /usr/local/elasticsearch/bin/elasticsearch-setup-passwords interactive
Initiating the setup of passwords for reserved users elastic,apm_system,kibana,logstash_system,beats_system,remote_monitoring_user.
You will be prompted to enter passwords as the process progresses.
Please confirm that you would like to continue [y/N]y


Enter password for [elastic]:				#配置密码
Reenter password for [elastic]:
Enter password for [apm_system]:
Reenter password for [apm_system]:
Enter password for [kibana]:
Reenter password for [kibana]:
Enter password for [logstash_system]:
Reenter password for [logstash_system]:
Enter password for [beats_system]:
Reenter password for [beats_system]:
Enter password for [remote_monitoring_user]:
Reenter password for [remote_monitoring_user]:
Changed password for user [apm_system]
Changed password for user [kibana]
Changed password for user [logstash_system]
Changed password for user [beats_system]
Changed password for user [remote_monitoring_user]
Changed password for user [elastic]
###

# 自动随机生成密码-方式二
[root@jenkins /usr/local]# /usr/local/elasticsearch7/bin/elasticsearch-setup-passwords auto --batch
Changed password for user apm_system
PASSWORD apm_system = LlqWxVcNDOLYKrYFkvYN

Changed password for user kibana_system
PASSWORD kibana_system = fQd1NkCemHVhhnYH9Ruy

Changed password for user kibana
PASSWORD kibana = fQd1NkCemHVhhnYH9Ruy

Changed password for user logstash_system
PASSWORD logstash_system = UIhrv3Yip3Fi9mMz6ASh

Changed password for user beats_system
PASSWORD beats_system = Yr4f87Mut81lT1VEWaQk

Changed password for user remote_monitoring_user
PASSWORD remote_monitoring_user = DDR6oMAjgTgIIQQuKULu

Changed password for user elastic
PASSWORD elastic = IYlnnaX1x4nnpMInthgy
###



# 访问elasticsearch进行服务测试
# 因为开启了xpack，所以这里无法直接访问
[opsuser@elasticsearch001 /usr/local/elasticsearch]$ curl 10.10.10.201:9200
{"error":{"root_cause":[{"type":"security_exception","reason":"missing authentication credentials for REST request [/]","header":{"WWW-Authenticate":"Basic realm=\"security\" charset=\"UTF-8\""}}],"type":"security_exception","reason":"missing authentication credentials for REST request [/]","header":{"WWW-Authenticate":"Basic realm=\"security\" charset=\"UTF-8\""}},"status":401}[opsuser@elasticsearch001 /usr/local/elasticsearch]$
# 使用elastic用户访问
[opsuser@elasticsearch001 /usr/local/elasticsearch]$ curl -u elastic 10.10.10.201:9200
Enter host password for user 'elastic':
{
  "name" : "hotel01",
  "cluster_name" : "hotelES",
  "cluster_uuid" : "McbApZiYT5CutpA-AZyOIg",
  "version" : {
    "number" : "7.7.1",
    "build_flavor" : "default",
    "build_type" : "tar",
    "build_hash" : "ad56dce891c901a492bb1ee393f12dfff473a423",
    "build_date" : "2020-05-28T16:30:01.040088Z",
    "build_snapshot" : false,
    "lucene_version" : "8.5.1",
    "minimum_wire_compatibility_version" : "6.8.0",
    "minimum_index_compatibility_version" : "6.0.0-beta1"
  },
  "tagline" : "You Know, for Search"
}
```





#### 2.3 Kibana

##### 2.3.1 安装服务

```bash
[opsuser@elasticsearch001 /usr/local/elasticsearch]$ cd /download/
[opsuser@elasticsearch001 /download]$ ls
elasticsearch-7.7.1-linux-x86_64.tar.gz  kibana-7.7.1-linux-x86_64.tar.gz
[opsuser@elasticsearch001 /download]$ sudo tar xf kibana-7.7.1-linux-x86_64.tar.gz -C /usr/local/
[opsuser@elasticsearch001 /download]$ cd /usr/local/
[opsuser@elasticsearch001 /usr/local]$ sudo ln -sv kibana-7.7.1-linux-x86_64/ kibana
[opsuser@elasticsearch001 /usr/local]$ sudo chown -R elasticsearch.elasticsearch /usr/local/kibana-7.7.1-linux-x86_64/
```



##### 2.3.2 配置服务

elasticsearch开启安全认证后，kibana连接elasticsearch以及访问elasticsearch都需要认证。

配置kibana和elasticsearch连接一共有两种方法，分别为明文和密文。



**明文配置(使用此配置)**

```bash
[opsuser@elasticsearch001 /usr/local]$ cd kibana
[opsuser@elasticsearch001 /usr/local/kibana]$ cat config/kibana.yml
server.port: 5601
server.host: "0.0.0.0"
server.name: "hotelui"
elasticsearch.hosts: ["http://127.0.0.1:9200"]
kibana.index: ".kibana"
i18n.locale: "zh-CN"
elasticsearch.username: "kibana"
elasticsearch.password: "password"
xpack.reporting.encryptionKey: "sSpUE8whw1eMnk2ISYjQeu4nKsXslDjz"       # 如果不添加这条配置，将会报错
xpack.security.encryptionKey: "yZr7lNijpHFb310qaEY5cp7MjVoyXw0C"        # 如果不配置这条，将会报错
```



**密文配置**

```bash
# 将账号和密码配置成secret
# 创建keystore文件
[opsuser@elasticsearch001 /usr/local/kibana]$ sudo -u elasticsearch /usr/local/kibana/bin/kibana-keystore --allow-root create
Created Kibana keystore in /usr/local/kibana-7.6.2-linux-x86_64/data/kibana.keystore
# 向此keystore添加账户
[opsuser@elasticsearch001 /usr/local/kibana]$ sudo -u elasticsearch /usr/local/kibana/bin/kibana-keystore --allow-root add elasticsearch.username
Enter value for elasticsearch.username: ******		# kibana
# 向此keystore添加密码
[opsuser@elasticsearch001 /usr/local/kibana]$ sudo -u elasticsearch /usr/local/kibana/bin/kibana-keystore --allow-root add elasticsearch.password
Enter value for elasticsearch.password: ******		# password

# 查看配置
[opsuser@elasticsearch001 /usr/local/kibana]$ cat config/kibana.yml
server.port: 5601
server.host: "0.0.0.0"
server.name: "hotelui"
elasticsearch.hosts: ["http://127.0.0.1:9200"]
kibana.index: ".kibana"
i18n.locale: "zh-CN"
xpack.reporting.encryptionKey: "sSpUE8whw1eMnk2ISYjQeu4nKsXslDjz"  #如果不添加这条配置，将会报错
xpack.security.encryptionKey: "yZr7lNijpHFb310qaEY5cp7MjVoyXw0C"   #如果不配置这条，将会报错
```



##### 2.3.3 启动服务

```bash
[opsuser@elasticsearch001 /usr/local/kibana]$ sudo cat /usr/lib/systemd/system/kibana.service
[Unit]
Description=https://elastic.co
After=network-online.target

[Service]
User=elasticsearch
Group=elasticsearch
Type=simple
ExecStart=/usr/local/kibana/bin/kibana
Restart=on-failure
LimitNOFILE=1000000
#MemoryLimit=1G
#MemoryAccounting=true

[Install]
WantedBy=multi-user.target


[opsuser@elasticsearch001 /usr/local/kibana]$ sudo systemctl daemon-reload
[opsuser@elasticsearch001 /usr/local/kibana]$ sudo systemctl start kibana.service
[opsuser@elasticsearch001 /usr/local/kibana]$ sudo systemctl status kibana.service
[opsuser@elasticsearch001 /usr/local/kibana]$ sudo systemctl enable kibana.service


# 查看服务使用的是node环境，无法限制内存
[opsuser@elasticsearch001 /usr/local/kibana]$ ps -ef | grep kibana
elastic+  3661     1  1 14:38 ?        00:00:58 /usr/local/kibana/bin/../node/bin/node /usr/local/kibana/bin/../src/cli
opsuser   4123  4003  0 16:13 pts/0    00:00:00 grep --color=auto kibana
```



##### 2.3.4 访问服务

http://10.10.10.201:5601

使用用户elastic用户访问



### 3. Elasticsearch数据恢复

前提：新旧版本必须一致，这里新旧版本皆为7.7.1



#### 3.1 备份指定索引

```bash
PUT _snapshot/restore_repo/snapshot_202407111348?wait_for_completion=true
{
    "indices": "interexpedia_region_ali_pro,interexpedia_regionen_ali_pro,interexpedia_hotelstatic_ali_pro_zhcn,interexpedia_hotelstatic_ali_pro_enus,interdaolvv2_facilities_db_ali_pro,interdaolvv2_hoteldescription_db_ali_pro,interdaolvv2_hotelareaattraction_db_ali_pro,interdaolvv2_policy_db_ali_pro,interdaolvv2_hotelstatic_db_ali_pro,interdaolvv2_roomtypeattribute_db_ali_pro,interdaolvv2_roomtype_db_ali_pro,interdaolvv2_hotelimage_db_ali_pro,interdaolvv2_roomimage_db_ali_pro,intercorev2_hotel_db_ali_pro,intercorev2_hotel_en_db_ali_pro,intercorev2_room_db_ali_pro,intercorev2_room_en_db_ali_pro,intercorev2_search_ali_pro,intercorev2_cityrelationchain_ali_pro,intercorev2_cityrelationbrand_ali_pro,intercorev2_cityrelationcategory_ali_pro,intercorev2_hotel_db_ali_pro_expedia,intercorev2_hotel_db_ali_pro_interdaolvv2"
}

# 备份所有索引
PUT _snapshot/restore_repo/snapshot_202410251420?wait_for_completion=true
```





#### 3.2 安装OSS插件

在所有Elasticsearch节点安装OSS插件，插件版本必须跟Elasticsearch版本一致

```bash
# 下载
[opsuser@elasticsearch001 /download]$ curl -OL https://github.com/aliyun/elasticsearch-repository-oss/releases/download/v7.7.1/elasticsearch-repository-oss-7.7.1.zip
[opsuser@elasticsearch001 /download]$ ll
total 586364
-rw-r--r-- 1 root root 314445622 Mar  8 11:30 elasticsearch-7.7.1-linux-x86_64.tar.gz
-rw-rw-r-- 1 root root   7018716 Mar  8 16:37 elasticsearch-repository-oss-7.7.1.zip
-rw-r--r-- 1 root root 278963930 Mar  8 11:31 kibana-7.7.1-linux-x86_64.tar.gz

# 将安装包解压到自建Elasticsearch各节点安装路径的plugins目录下
[opsuser@elasticsearch001 /download]$ sudo unzip -d /usr/local/elasticsearch-7.7.1/plugins/elasticsearch-repository-oss elasticsearch-repository-oss-7.7.1.zip
# 或者使用此命令安装
[opsuser@elasticsearch001 /download]$ sudo /usr/local/elasticsearch-7.7.1/bin/elasticsearch-plugin install file:///download/elasticsearch-repository-oss-7.7.1.zip
# 查看安装的插件
[opsuser@elasticsearch001 /download]$ ls /usr/local/elasticsearch-7.7.1/plugins/elasticsearch-repository-oss

# 重启自建Elasticsearch集群各节点服务，滚动方式重启，不影响服务
[opsuser@elasticsearch001 /download]$ sudo systemctl restart elasticsearch.service
```



#### 3.3 创建仓库

```bash
# 查看所有仓库
GET _snapshot
---
{ }

# 创建仓库
PUT _snapshot/restore_repo/
{
    "type": "oss",
    "settings": {
        "endpoint": "http://oss-cn-shanghai-internal.aliyuncs.com",
        "access_key_id": "AK",
        "secret_access_key": "SK",
        "bucket": "${BUCKET_NAME}",
        "compress": true,
        "chunk_size": "500mb",
        "base_path": "snapshot/"
    }
}
---
{
  "acknowledged" : true
}

# 再次查看所有仓库
GET _snapshot
---
{
  "restore_repo" : {
    "type" : "oss",
    "settings" : {
      "bucket" : "PREFIX-aliyun-elasticsearch-backup",
      "base_path" : "snapshot/",
      "chunk_size" : "500mb",
      "endpoint" : "http://oss-cn-shanghai-internal.aliyuncs.com",
      "compress" : "true"
    }
  }
}

# 查看特定仓库所有快照信息
GET _snapshot/restore_repo/_all
---
{
  "snapshots" : [
    {
      "snapshot" : "snapshot_1",		# 为快照名称
      "uuid" : "9-mRBJaIRgGmXggzUr_4MA",
      "version_id" : 7070199,
      "version" : "7.7.1",
      "indices" : [
        "dongchenghotelprice_db_pro_ali_v2_hszl_1e16776599c97bc78fe21b7dce74494d",
        "interdaolv_roomtypeattribute_db_ali_v1",
        "corehotel_db_pro_ali_alibtrip",
        ....
		"daolv_hoteldescription_db_ali",
        "elonghotelroom_db_en_pro"
      ],
      "include_global_state" : true,
      "state" : "SUCCESS",
      "start_time" : "2024-03-06T07:28:37.716Z",
      "start_time_in_millis" : 1709710117716,
      "end_time" : "2024-03-06T08:46:32.207Z",
      "end_time_in_millis" : 1709714792207,
      "duration_in_millis" : 4674491,
      "failures" : [ ],
      "shards" : {
        "total" : 1214,
        "failed" : 0,
        "successful" : 1214
      }
    }
  ]
}
```



#### 3.4 恢复快照数据

```bash
# 恢复指定快照中的指定索引
POST /_snapshot/restore_repo/snapshot_1/_restore
{
 "indices": "corehotel_db_pro_ali_alibtrip", 
 "ignore_unavailable":"true"
}
---
{
  "accepted" : true
}

# 查看恢复的索引状态为red，因为这里部署的是单节点，而恢复的索引都有副本分片，因此为red状态，可以在恢复的时候将副本分布配置为0
POST /_snapshot/restore_repo/snapshot_1/_restore
{
 "indices": "corehotel_db_pro_ali_alibtrip",
 "ignore_unavailable":"true",
 "index_settings": {
    "index.number_of_replicas": 0
  }
}


# 手动将特定索引副本分片设置为0
PUT /corehotel_db_pro_ali_alibtrip/_settings
{
    "index": {
        "number_of_replicas" : 0
    }
}
--- # 此时报错，提示[默认]的分析器类型[ik_max_word]未知，缺少analysis-ik插件
{
  "error" : {
    "root_cause" : [
      {
        "type" : "illegal_argument_exception",
        "reason" : "Unknown analyzer type [ik_max_word] for [default]"
      }
    ],
    "type" : "illegal_argument_exception",
    "reason" : "Unknown analyzer type [ik_max_word] for [default]"
  },
  "status" : 400
}


# 插件问题处理
# 下载analysis-ik分词器
[opsuser@elasticsearch001 /download]$ sudo curl -OL https://github.com/infinilabs/analysis-ik/releases/download/v7.7.1/elasticsearch-analysis-ik-7.7.1.zip
[opsuser@elasticsearch001 /download]$ ll elasticsearch-analysis-ik-7.7.1.zip
-rw-rw-r-- 1 root root 4504441 Mar  8 17:09 elasticsearch-analysis-ik-7.7.1.zip

# 安装analysis-ik分词器
[opsuser@elasticsearch001 /download]$ sudo /usr/local/elasticsearch-7.7.1/bin/elasticsearch-plugin install file:///download/elasticsearch-analysis-ik-7.7.1.zip
-> Installing file:///download/elasticsearch-analysis-ik-7.7.1.zip
-> Downloading file:///download/elasticsearch-analysis-ik-7.7.1.zip
[=================================================] 100%
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@     WARNING: plugin requires additional permissions     @
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
* java.net.SocketPermission * connect,resolve
See http://docs.oracle.com/javase/8/docs/technotes/guides/security/permissions.html
for descriptions of what these permissions allow and the associated risks.

Continue with installation? [y/N]y
-> Installed analysis-ik
# 查看插件
[opsuser@elasticsearch001 /download]$ ls /usr/local/elasticsearch-7.7.1/plugins/
analysis-ik  elasticsearch-repository-oss
# 重启服务
[opsuser@elasticsearch001 /download]$ sudo systemctl restart elasticsearch.service


# 此时再更改副本分片配置，将不会报analysis-ik的错
PUT /corehotel_db_pro_ali_alibtrip/_settings
{
    "index": {
        "number_of_replicas" : 0
    }
}
---
{
  "acknowledged" : true
}

# 如果集群中已有同名称的索引将会报错
[restore_repo:snapshot_1/9-mRBJaIRgGmXggzUr_4MA] cannot restore index [corehotel_db_pro_ali_hrs] because an open index with same name already exists in the cluster. Either close or delete the existing index or restore the index under a different name by providing a rename pattern and replacement name
---英译
[restore_repo:snapshot_1/9-mRBJaIRgGmXggzUr_4MA]无法还原索引 [corehotel_db_pro_ali_hrs]，因为集群中已存在同名的开放索引。要么关闭或删除现有索引，要么通过提供重命名模式和替换名称以不同名称恢复索引




# 恢复所有快照数据，会报隐藏索引同名错误
POST /_snapshot/restore_repo/snapshot_1/_restore
{
 "ignore_unavailable":"true",
 "index_settings": {
    "index.number_of_replicas": 0
  }
}

# 或者用此配置恢复除隐藏索引外的所有索引
POST /_snapshot/restore_repo/snapshot_1/_restore
{
  "indices":"*,-.reporting*,-.security*,-.kibana*,-.apm*,-.async*",
  "ignore_unavailable":"true",
  "index_settings": {
    "index.number_of_replicas": 0
  }
}


# 查看恢复状态
GET /_recovery
GET /_cat/recovery
GET /interdaolvv2_hotelstatic_db_ali_pro/_recovery

# 查看是否恢复完成,stage=done表示已完成,stage=index表示正在索引中
[ops0799@interES /download/hoteles]$ curl -s -u ops0799 http://localhost:9200/_cat/recovery?v | grep index
Enter host password for user 'ops0799':
index                                       shard time  type           stage source_host source_node target_host  target_node  repository   snapshot              files files_recovered files_percent files_total bytes      bytes_recovered bytes_percent bytes_total translog_ops translog_ops_recovered translog_ops_percent
interexpedia_hotelstatic_ali_pro_enus       0     1.6m  snapshot       index n/a         n/a         10.10.10.204 interhotel01 restore_repo snapshot_202407111356 148   146             98.6%         148         4012104617 3118954440      77.7%         4012104617  0            0                      100.0%
interexpedia_hotelstatic_ali_pro_enus       1     1.5m  snapshot       index n/a         n/a         10.10.10.204 interhotel01 restore_repo snapshot_202407111356 118   44              37.3%         118         3557958752 415589913       11.7%         3557958752  0            0                      100.0%
interexpedia_hotelstatic_ali_pro_enus       2     1.4m  snapshot       index n/a         n/a         10.10.10.204 interhotel01 restore_repo snapshot_202407111356 133   0               0.0%          133         3777676496 0          0.0%          3777676496  0            0                      100.0%
interexpedia_hotelstatic_ali_pro_enus       3     1.3m  snapshot       index n/a         n/a         10.10.10.204 interhotel01 restore_repo snapshot_202407111356 0     0               0.0%          0           0          0          0.0%          0           0            0                      100.0%
```







## Harbor



环境：

docker 20.10.10-ce+ and docker-compose 1.18.0+ 

[harbor-offline-installer-v2.9.4.tgz](https://github.com/goharbor/harbor/releases/download/v2.9.4/harbor-offline-installer-v2.9.4.tgz)



### 1. 安装docker

```bash
[user@harbor ~]$ sudo yum install -y yum-utils device-mapper-persistent-data lvm2
[user@harbor ~]$ sudo yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
[user@harbor ~]$ sudo sed -i 's+download.docker.com+mirrors.aliyun.com/docker-ce+' /etc/yum.repos.d/docker-ce.repo
[user@harbor ~]$ sudo yum makecache fast
[user@harbor ~]$ sudo yum -y install docker-ce
[user@harbor ~]$ sudo systemctl enable docker.service
[user@harbor ~]$ sudo systemctl start docker.service
[user@harbor ~]$ sudo docker version
Client: Docker Engine - Community
 Version:           26.1.2
 API version:       1.45
 Go version:        go1.21.10
 Git commit:        211e74b
 Built:             Wed May  8 14:01:02 2024
 OS/Arch:           linux/amd64
 Context:           default

Server: Docker Engine - Community
 Engine:
  Version:          26.1.2
  API version:      1.45 (minimum version 1.24)
  Go version:       go1.21.10
  Git commit:       ef1912d
  Built:            Wed May  8 13:59:55 2024
  OS/Arch:          linux/amd64
  Experimental:     false
 containerd:
  Version:          1.6.31
  GitCommit:        e377cd56a71523140ca6ae87e30244719194a521
 runc:
  Version:          1.1.12
  GitCommit:        v1.1.12-0-g51d5e94
 docker-init:
  Version:          0.19.0
  GitCommit:        de40ad0
```



### 2. 安装docker-compose

```bash
[user@harbor /download]$ curl -OL https://github.com/docker/compose/releases/download/v2.27.0/docker-compose-linux-x86_64
[user@harbor /download]$ ls | grep docker-compose
docker-compose-linux-x86_64
[user@harbor /download]$ sudo cp docker-compose-linux-x86_64 /usr/local/bin/docker-compose
[user@harbor /download]$ sudo chmod o+x /usr/local/bin/docker-compose
[user@harbor /download]$ docker-compose version
Docker Compose version v2.27.0
```



### 3. 下载安装程序

```bash
[user@harbor ~]$ sudo mkdir -p /download
[user@harbor ~]$ cd /download/
# 在线安装包
[user@harbor /download]$ sudo curl -OL https://github.com/goharbor/harbor/releases/download/v2.9.4/harbor-online-installer-v2.9.4.tgz
[user@harbor /download]$ cd harbor/
[user@harbor /download]$ sudo tar xf harbor-online-installer-v2.9.4.tgz
[user@harbor /download/harbor]$ ls
common.sh  harbor.yml.tmpl  install.sh  LICENSE  prepare

```



### 4. 配置harbor

```bash
[user@harbor /download]$ sudo mv harbor /usr/local/
[user@harbor /download]$ sudo cd /usr/local/harbor
[user@harbor /usr/local/harbor]$ sudo mkdir -p /data/harbor
[user@harbor /usr/local/harbor]$ sudo mkdir -p /data/harbor/log
[user@harbor /usr/local/harbor]$ sudo cp harbor.yml.tmpl harbor.yml
[user@harbor /usr/local/harbor]$ sudo vim harbor.yml
[user@harbor /usr/local/harbor]$ grep -Ev '#|^$' harbor.yml
hostname: harbor.homsom.com
http:
  port: 80
https:
  port: 443
  certificate: /usr/local/harbor/cert/homsom.com.pem
  private_key: /usr/local/harbor/cert/homsom.com.key
harbor_admin_password: NoJ3bCFIA1gEQDFT
database:
  password: elctUegX
  max_idle_conns: 100
  max_open_conns: 900
  conn_max_lifetime: 5m
  conn_max_idle_time: 0
data_volume: /data/harbor
trivy:
  ignore_unfixed: false
  skip_update: false
  skip_java_db_update: false
  offline_scan: false
  security_check: vuln
  insecure: false
jobservice:
  max_job_workers: 20
  job_loggers:
    - STD_OUTPUT
    - FILE
notification:
  webhook_job_max_retry: 3
log:
  level: info
  local:
    rotate_count: 50
    rotate_size: 200M
    location: /data/harbor/log/harbor
_version: 2.9.0
proxy:
  http_proxy:
  https_proxy:
  no_proxy:
  components:
    - core
    - jobservice
    - trivy
upload_purging:
  enabled: true
  age: 168h
  interval: 24h
  dryrun: false
cache:
  enabled: false
  expire_hours: 24
```



### 5. 安装harbor

```bash
# trivy 默认是关闭的，如需要开启，请更改with_trivy=$true
with_trivy=$false

# 安装
[user@harbor /usr/local/harbor]$ sudo ./install.sh
[user@harbor /usr/local/harbor]$ sudo ./install.sh

[Step 0]: checking if docker is installed ...

Note: docker version: 26.1.2

[Step 1]: checking docker-compose is installed ...

Note: Docker Compose version v2.27.0


[Step 2]: preparing environment ...

[Step 3]: preparing harbor configs ...
prepare base dir is set to /usr/local/harbor
Unable to find image 'goharbor/prepare:v2.9.4' locally
v2.9.4: Pulling from goharbor/prepare
5c70ea440659: Pull complete
97b717f3829c: Pull complete
8e3e2393f1f5: Pull complete
922fd4ce5cf9: Pull complete
6d5dc2e2a2f6: Pull complete
2e06411d314c: Pull complete
b129f0ef1377: Pull complete
4dcbfb4c87cd: Pull complete
792e707a745f: Pull complete
c25b0082aebd: Pull complete
Digest: sha256:8525fb155c3471a624b9e736a1ab26298e574450f045a96efd85862e8a873711
Status: Downloaded newer image for goharbor/prepare:v2.9.4
Generated configuration file: /config/portal/nginx.conf
Generated configuration file: /config/log/logrotate.conf
Generated configuration file: /config/log/rsyslog_docker.conf
Generated configuration file: /config/nginx/nginx.conf
Generated configuration file: /config/core/env
Generated configuration file: /config/core/app.conf
Generated configuration file: /config/registry/config.yml
Generated configuration file: /config/registryctl/env
Generated configuration file: /config/registryctl/config.yml
Generated configuration file: /config/db/env
Generated configuration file: /config/jobservice/env
Generated configuration file: /config/jobservice/config.yml
Generated and saved secret to file: /data/secret/keys/secretkey
Successfully called func: create_root_cert
Generated configuration file: /compose_location/docker-compose.yml
Clean up the input dir


Note: stopping existing Harbor instance ...
WARN[0000] /usr/local/harbor/docker-compose.yml: `version` is obsolete


[Step 4]: starting Harbor ...
WARN[0000] /usr/local/harbor/docker-compose.yml: `version` is obsolete
[+] Running 62/31
 ✔ redis Pulled                                                                                                                                                                                                104.7s
 ✔ jobservice Pulled                                                                                                                                                                                            57.6s
 ✔ proxy Pulled                                                                                                                                                                                                 44.1s
 ✔ log Pulled                                                                                                                                                                                                  110.2s
 ✔ portal Pulled                                                                                                                                                                                                65.7s
 ✔ core Pulled                                                                                                                                                                                                  18.0s
 ✔ registryctl Pulled                                                                                                                                                                                           28.9s
 ✔ registry Pulled                                                                                                                                                                                              40.0s
 ✔ postgresql Pulled                                                                                                                                                                                           106.6s
 
[+] Running 10/10
 ✔ Network harbor_harbor        Created                                                                                                                                                                          0.0s
 ✔ Container harbor-log         Started                                                                                                                                                                          5.7s
 ✔ Container redis              Started                                                                                                                                                                          0.8s
 ✔ Container registryctl        Started                                                                                                                                                                          0.8s
 ✔ Container harbor-db          Started                                                                                                                                                                          0.7s
 ✔ Container harbor-portal      Started                                                                                                                                                                          0.7s
 ✔ Container registry           Started                                                                                                                                                                          0.8s
 ✔ Container harbor-core        Started                                                                                                                                                                          0.8s
 ✔ Container nginx              Started                                                                                                                                                                          1.0s
 ✔ Container harbor-jobservice  Started                                                                                                                                                                          1.0s
✔ ----Harbor has been installed and started successfully.---- 

# 查看运行状态
[user@harbor /usr/local/harbor]$ sudo docker ps -a
CONTAINER ID   IMAGE                                COMMAND                  CREATED          STATUS                    PORTS                                         NAMES
77d2cdc3b5c4   goharbor/harbor-jobservice:v2.9.4    "/harbor/entrypoint.…"   43 seconds ago   Up 37 seconds (healthy)                                                 harbor-jobservice
ace876fb24a0   goharbor/nginx-photon:v2.9.4         "nginx -g 'daemon of…"   43 seconds ago   Up 42 seconds (healthy)   0.0.0.0:80->8080/tcp, 0.0.0.0:443->8443/tcp   nginx
ed47076f235d   goharbor/harbor-core:v2.9.4          "/harbor/entrypoint.…"   43 seconds ago   Up 42 seconds (healthy)                                                 harbor-core
ff003eb4d605   goharbor/harbor-portal:v2.9.4        "nginx -g 'daemon of…"   43 seconds ago   Up 42 seconds (healthy)                                                 harbor-portal
5296708c8af4   goharbor/harbor-db:v2.9.4            "/docker-entrypoint.…"   43 seconds ago   Up 42 seconds (healthy)                                                 harbor-db
eca62bfcd0ab   goharbor/registry-photon:v2.9.4      "/home/harbor/entryp…"   43 seconds ago   Up 42 seconds (healthy)                                                 registry
8fc7fbd03b1a   goharbor/redis-photon:v2.9.4         "redis-server /etc/r…"   43 seconds ago   Up 42 seconds (healthy)                                                 redis
c58ff7465eda   goharbor/harbor-registryctl:v2.9.4   "/home/harbor/start.…"   43 seconds ago   Up 42 seconds (healthy)                                                 registryctl
884b393c5702   goharbor/harbor-log:v2.9.4           "/bin/sh -c /usr/loc…"   48 seconds ago   Up 42 seconds (healthy)   127.0.0.1:1514->10514/tcp                     harbor-log
```



### 6. 配置harbor自动启动

```bash
[user@harbor /usr/local/harbor]$ sudo vim /usr/lib/systemd/system/harbor.service
[user@harbor /usr/local/harbor]$ sudo systemctl cat harbor.service
# /usr/lib/systemd/system/harbor.service
[Unit]
Description=Harbor
After=docker.service systemd-networkd.service systemd-resolved.service
Requires=docker.service
Documentation=http://github.com/vmware/harbor

[Service]
Type=simple
Restart=on-failure
RestartSec=5
ExecStart=/usr/local/bin/docker-compose -f /usr/local/harbor/docker-compose.yml up
ExecStop=/usr/local/bin/docker-compose -f /usr/local/harbor/docker-compose.yml down

[Install]
WantedBy=multi-user.target

# 启动
[user@harbor /usr/local/harbor]$ sudo systemctl daemon-reload
[user@harbor /usr/local/harbor]$ sudo systemctl enable harbor.service
[user@harbor /usr/local/harbor]$ sudo systemctl start harbor.service
[user@harbor /usr/local/harbor]$ sudo systemctl status harbor.service
● harbor.service - Harbor
   Loaded: loaded (/usr/lib/systemd/system/harbor.service; disabled; vendor preset: disabled)
   Active: active (running) since Fri 2024-05-10 17:37:25 CST; 1s ago
     Docs: http://github.com/vmware/harbor
 Main PID: 16798 (docker-compose)
    Tasks: 13
   Memory: 14.8M
   CGroup: /system.slice/harbor.service
           └─16798 /usr/local/bin/docker-compose -f /usr/local/harbor/docker-compose.yml up

# 停止
[user@harbor /usr/local/harbor]$ sudo systemctl stop harbor.service
[user@harbor /usr/local/harbor]$ sudo systemctl status harbor.service
● harbor.service - Harbor
   Loaded: loaded (/usr/lib/systemd/system/harbor.service; disabled; vendor preset: disabled)
   Active: inactive (dead)
     Docs: http://github.com/vmware/harbor

May 10 17:37:41 harbor.ops.hs.com docker-compose[16798]: [27B blob data]
May 10 17:37:41 harbor.ops.hs.com docker-compose[17681]: Container redis  Removed
May 10 17:37:41 harbor.ops.hs.com docker-compose[17681]: Container harbor-log  Stopping
May 10 17:37:51 harbor.ops.hs.com docker-compose[17681]: Container harbor-log  Stopped
May 10 17:37:51 harbor.ops.hs.com docker-compose[17681]: Container harbor-log  Removing
May 10 17:37:51 harbor.ops.hs.com docker-compose[16798]: [34B blob data]
May 10 17:37:51 harbor.ops.hs.com docker-compose[17681]: Container harbor-log  Removed
May 10 17:37:51 harbor.ops.hs.com docker-compose[17681]: Network harbor_harbor  Removing
May 10 17:37:51 harbor.ops.hs.com docker-compose[17681]: Network harbor_harbor  Removed
May 10 17:37:51 harbor.ops.hs.com systemd[1]: Stopped Harbor.
           
```































































## 阿里云自建K8S



**资源清单**

| IP          | Role         |
| ----------- | ------------ |
| 172.16.0.10 | NAT、DNS     |
| 172.16.0.14 | ansible      |
| 172.16.0.11 | k8s-master   |
| 172.16.0.12 | k8s-worker01 |
| 172.16.0.13 | k8s-worker02 |



NAT、DNS部署省略



### 1. 部署kubeasz



#### 1.1 安装ansible

```bash
# 安装ansible
root@ansible:~/ansible# apt install -y ansible python3-pip sshpass

# 配置免密
root@ansible:~/ansible# cat ssh-cp.sh 
#!/bin/bash
#Destination Host Ip
IP="
172.16.0.10
172.16.0.11
172.16.0.12
172.16.0.13
172.16.0.14
"
for node in ${IP};do
        #sshpass -p 123456 ssh-copy-id ${node} -o StrictHostKeyChecking=no
        sshpass ssh-copy-id ${node} -o StrictHostKeyChecking=no
        if [ $? -eq 0 ];then
                echo "${node} secret copy successful."
        else
                echo "${node} secret copy failed."
        fi
done

# 配置ansible hosts
root@ansible:~/ansible# cat /etc/ansible/hosts
[nat]
172.16.0.10

[ansible]
172.16.0.14

[k8s]
172.16.0.11
172.16.0.12
172.16.0.13
---
```



#### 1.2 安装kubeasz

```bash
root@ansible:~/ansible# export release=3.2.0
root@ansible:~/ansible# wget https://github.com/easzlab/kubeasz/releases/download/${release}/ezdown
root@ansible:~/ansible# chmod +x ./ezdown

# 查看将要安装的docker和k8s版本
root@ansible:~/ansible# grep -E 'DOCKER_VER=|K8S_BIN_VER=' ezdown 
DOCKER_VER=20.10.9
K8S_BIN_VER=v1.23.1
          DOCKER_VER="$OPTARG"
          K8S_BIN_VER="$OPTARG"
# 更改将要安装的docker和k8s版本，为自己需要的版本
root@ansible:~/ansible# grep -E 'DOCKER_VER=|K8S_BIN_VER=' ezdown 
DOCKER_VER=19.03.15
K8S_BIN_VER=v1.23.17
          DOCKER_VER="$OPTARG"
          K8S_BIN_VER="$OPTARG"

# 下载所需要的所有文件
root@ansible:~/ansible# ./ezdown -D
2024-03-09 18:21:26 INFO Action successed: download_all

# 下载完成后的目录及镜像状态
root@ansible:~/ansible# ls -l /etc/kubeasz/
total 108
-rw-rw-r--  1 root root 20304 Jan  5  2022 ansible.cfg
drwxr-xr-x  3 root root  4096 Mar  9 18:15 bin
drwxrwxr-x  8 root root  4096 Jan  5  2022 docs
drwxr-xr-x  2 root root  4096 Mar  9 18:21 down
drwxrwxr-x  2 root root  4096 Jan  5  2022 example
-rwxrwxr-x  1 root root 24716 Jan  5  2022 ezctl
-rwxrwxr-x  1 root root 15350 Jan  5  2022 ezdown
drwxrwxr-x 10 root root  4096 Jan  5  2022 manifests
drwxrwxr-x  2 root root  4096 Jan  5  2022 pics
drwxrwxr-x  2 root root  4096 Jan  5  2022 playbooks
-rw-rw-r--  1 root root  6137 Jan  5  2022 README.md
drwxrwxr-x 22 root root  4096 Jan  5  2022 roles
drwxrwxr-x  2 root root  4096 Jan  5  2022 tools
root@ansible:~/ansible# docker  image ls 
REPOSITORY                                TAG                 IMAGE ID            CREATED             SIZE
easzlab/kubeasz-k8s-bin                   v1.23.17            d96f7551a751        11 months ago       1.17GB
easzlab/kubeasz                           3.2.0               cbf7e17a5ddb        2 years ago         164MB
easzlab/kubeasz-ext-bin                   1.0.0               ce7839fb3045        2 years ago         424MB
easzlab/metrics-server                    v0.5.2              f965999d664b        2 years ago         64.3MB
easzlab/flannel                           v0.15.1             e6ea68648f0c        2 years ago         69.5MB
kubernetesui/dashboard                    v2.4.0              72f07539ffb5        2 years ago         221MB
coredns/coredns                           1.8.6               a4ca41631cc7        2 years ago         46.8MB
easzlab/k8s-dns-node-cache                1.21.1              5bae806f8f12        2 years ago         104MB
calico/node                               v3.19.3             5ee77fcf72b4        2 years ago         154MB
calico/pod2daemon-flexvol                 v3.19.3             c34539dc5eef        2 years ago         21.7MB
calico/cni                                v3.19.3             71da92beaffe        2 years ago         146MB
calico/kube-controllers                   v3.19.3             ed1e25e26ab5        2 years ago         60.6MB
easzlab/pause                             3.6                 6270bb605e12        2 years ago         683kB
kubernetesui/metrics-scraper              v1.0.7              7801cfc6d5c0        2 years ago         34.4MB
easzlab/nfs-subdir-external-provisioner   v4.0.2              cf366ff50762        2 years ago         43.8MB

# 配置环境变量
root@ansible:~/ansible# cat >> /etc/profile.d/kubeasz << EOF
export PATH=$PATH:/etc/kubeasz/
EOF
root@ansible:~/ansible# source /etc/profile
```



### 2. 部署k8s集群-kubeasz



#### 2.1 创建k8s集群

```bash
root@ansible:~/ansible# ezctl new k8s01 
2024-03-09 19:04:44 DEBUG generate custom cluster files in /etc/kubeasz/clusters/k8s01
2024-03-09 19:04:44 DEBUG set versions
2024-03-09 19:04:44 DEBUG cluster k8s01: files successfully created.
2024-03-09 19:04:44 INFO next steps 1: to config '/etc/kubeasz/clusters/k8s01/hosts'
2024-03-09 19:04:44 INFO next steps 2: to config '/etc/kubeasz/clusters/k8s01/config.yml'

root@ansible:~/ansible# ls /etc/kubeasz/clusters/k8s01/
config.yml  hosts
root@ansible:~/ansible# cd /etc/kubeasz/clusters/k8s01/
```



#### 2.2 配置k8s01集群

```bash
# 配置主机，配置各角色下的主机地址
root@ansible:/etc/kubeasz/clusters/k8s01# grep -Ev '#|^$' hosts
[etcd]
172.16.0.11
[kube_master]
172.16.0.11
[kube_node]
172.16.0.12
172.16.0.13
[harbor]
[ex_lb]
[chrony]
[all:vars]
SECURE_PORT="6443"
CONTAINER_RUNTIME="docker"			# 容器运行时使用docker
CLUSTER_NETWORK="flannel"			# 公有云上使用flannel，本地网络使用calico
PROXY_MODE="ipvs"
SERVICE_CIDR="10.68.0.0/16"
CLUSTER_CIDR="172.20.0.0/16"
NODE_PORT_RANGE="30000-52767"		# NodePort范围
CLUSTER_DNS_DOMAIN="cluster.local"
bin_dir="/opt/kube/bin"
base_dir="/etc/kubeasz"
cluster_dir="{{ base_dir }}/clusters/k8s01"
ca_dir="/etc/kubernetes/ssl"


# 配置config.yml
root@ansible:/etc/kubeasz/clusters/k8s01# grep -Ev '#|^$' config.yml 
INSTALL_SOURCE: "online"
OS_HARDEN: false
ntp_servers:
  - "ntp.aliyun.com"
  - "time1.cloud.tencent.com"
  - "0.cn.pool.ntp.org"
local_network: "0.0.0.0/0"
CA_EXPIRY: "876000h"
CERT_EXPIRY: "876000h"
CLUSTER_NAME: "cluster1"
CONTEXT_NAME: "context-{{ CLUSTER_NAME }}"
K8S_VER: "1.23.17"
ETCD_DATA_DIR: "/var/lib/etcd"
ETCD_WAL_DIR: ""
ENABLE_MIRROR_REGISTRY: true
SANDBOX_IMAGE: "easzlab/pause:3.6"
CONTAINERD_STORAGE_DIR: "/var/lib/containerd"
DOCKER_STORAGE_DIR: "/var/lib/docker"
ENABLE_REMOTE_API: false
INSECURE_REG: '["127.0.0.1/8","harbor.markli.cn","repo.markli.cn"]'
MASTER_CERT_HOSTS:
  - "172.16.0.100"
  - "172.16.0.11"
  - "apiserver.jack.com"
  - "k8s-master.jack.com"
  - "apiserver.markli.cn"
  - "k8s-master.markli.cn"
NODE_CIDR_LEN: 24
KUBELET_ROOT_DIR: "/var/lib/kubelet"
MAX_PODS: 200
KUBE_RESERVED_ENABLED: "no"
SYS_RESERVED_ENABLED: "no"
BALANCE_ALG: "roundrobin"
FLANNEL_BACKEND: "vxlan"
DIRECT_ROUTING: false
flannelVer: "v0.15.1"
flanneld_image: "easzlab/flannel:{{ flannelVer }}"
flannel_offline: "flannel_{{ flannelVer }}.tar"
CALICO_IPV4POOL_IPIP: "Always"
IP_AUTODETECTION_METHOD: "can-reach={{ groups['kube_master'][0] }}"
CALICO_NETWORKING_BACKEND: "brid"
calico_ver: "v3.19.3"
calico_ver_main: "{{ calico_ver.split('.')[0] }}.{{ calico_ver.split('.')[1] }}"
calico_offline: "calico_{{ calico_ver }}.tar"
ETCD_CLUSTER_SIZE: 1
cilium_ver: "v1.4.1"
cilium_offline: "cilium_{{ cilium_ver }}.tar"
OVN_DB_NODE: "{{ groups['kube_master'][0] }}"
kube_ovn_ver: "v1.5.3"
kube_ovn_offline: "kube_ovn_{{ kube_ovn_ver }}.tar"
OVERLAY_TYPE: "full"
FIREWALL_ENABLE: "true"
kube_router_ver: "v0.3.1"
busybox_ver: "1.28.4"
kuberouter_offline: "kube-router_{{ kube_router_ver }}.tar"
busybox_offline: "busybox_{{ busybox_ver }}.tar"
dns_install: "yes"
corednsVer: "1.8.6"
ENABLE_LOCAL_DNS_CACHE: true
dnsNodeCacheVer: "1.21.1"
LOCAL_DNS_CACHE: "169.254.20.10"
metricsserver_install: "yes"
metricsVer: "v0.5.2"
dashboard_install: "no"
dashboardVer: "v2.4.0"
dashboardMetricsScraperVer: "v1.0.7"
ingress_install: "no"
ingress_backend: "traefik"
traefik_chart_ver: "10.3.0"
prom_install: "no"
prom_namespace: "monitor"
prom_chart_ver: "12.10.6"
nfs_provisioner_install: "no"
nfs_provisioner_namespace: "kube-system"
nfs_provisioner_ver: "v4.0.2"
nfs_storage_class: "managed-nfs-storage"
nfs_server: "192.168.1.10"
nfs_path: "/data/nfs"
HARBOR_VER: "v2.1.3"
HARBOR_DOMAIN: "harbor.yourdomain.com"
HARBOR_TLS_PORT: 8443
HARBOR_SELF_SIGNED_CERT: true
HARBOR_WITH_NOTARY: false
HARBOR_WITH_TRIVY: false
HARBOR_WITH_CLAIR: false
HARBOR_WITH_CHARTMUSEUM: true

```



#### 2.3 部署k8s集群

```bash
# 执行系统初始化
root@ansible:/etc/kubeasz/clusters/k8s01# ezctl setup k8s01 01 

# 部署etcd
root@ansible:/etc/kubeasz/clusters/k8s01# ezctl setup k8s01 02 

# 部署容器运行时环境
root@ansible:/etc/kubeasz/clusters/k8s01# ezctl setup k8s01 03

# 部署master
root@ansible:/etc/kubeasz/clusters/k8s01# ezctl setup k8s01 04

# 部署node
root@ansible:/etc/kubeasz/clusters/k8s01# ezctl setup k8s01 05

# 部署网络
root@ansible:/etc/kubeasz/clusters/k8s01# ezctl setup k8s01 06 
# 查看节点路由网络
# k8s-master
root@k8s001:~# route -n 
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         172.16.0.253    0.0.0.0         UG    100    0        0 eth0
172.16.0.0      0.0.0.0         255.255.255.0   U     0      0        0 eth0
172.16.0.253    0.0.0.0         255.255.255.255 UH    100    0        0 eth0
172.17.0.0      0.0.0.0         255.255.0.0     U     0      0        0 docker0
172.20.1.0      172.20.1.0      255.255.255.0   UG    0      0        0 flannel.1
172.20.2.0      172.20.2.0      255.255.255.0   UG    0      0        0 flannel.1
# k8s-worker01
root@k8s002:~# route -n
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         172.16.0.253    0.0.0.0         UG    100    0        0 eth0
172.16.0.0      0.0.0.0         255.255.255.0   U     0      0        0 eth0
172.16.0.253    0.0.0.0         255.255.255.255 UH    100    0        0 eth0
172.17.0.0      0.0.0.0         255.255.0.0     U     0      0        0 docker0
172.20.0.0      172.20.0.0      255.255.255.0   UG    0      0        0 flannel.1
172.20.1.0      0.0.0.0         255.255.255.0   U     0      0        0 cni0
172.20.2.0      172.20.2.0      255.255.255.0   UG    0      0        0 flannel.1
# k8s-worker02
root@k8s003:~# route -n
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         172.16.0.253    0.0.0.0         UG    100    0        0 eth0
172.16.0.0      0.0.0.0         255.255.255.0   U     0      0        0 eth0
172.16.0.253    0.0.0.0         255.255.255.255 UH    100    0        0 eth0
172.17.0.0      0.0.0.0         255.255.0.0     U     0      0        0 docker0
172.20.0.0      172.20.0.0      255.255.255.0   UG    0      0        0 flannel.1
172.20.1.0      172.20.1.0      255.255.255.0   UG    0      0        0 flannel.1
172.20.2.0      0.0.0.0         255.255.255.0   U     0      0        0 cni0

# 查看集群node和pod运行情况
root@ansible:~# kubectl get nodes
NAME          STATUS                     ROLES    AGE     VERSION
172.16.0.11   Ready,SchedulingDisabled   master   3m58s   v1.23.17
172.16.0.12   Ready                      node     2m19s   v1.23.17
172.16.0.13   Ready                      node     2m19s   v1.23.17
root@ansible:~# kubectl get pods -A -o wide 
NAMESPACE     NAME                    READY   STATUS    RESTARTS   AGE   IP            NODE          NOMINATED NODE   READINESS GATES
kube-system   kube-flannel-ds-n9nts   1/1     Running   0          63s   172.16.0.11   172.16.0.11   <none>           <none>
kube-system   kube-flannel-ds-p9kl2   1/1     Running   0          63s   172.16.0.12   172.16.0.12   <none>           <none>
kube-system   kube-flannel-ds-zd988   1/1     Running   0          63s   172.16.0.13   172.16.0.13   <none>           <none>



# 部署集群插件
root@ansible:/etc/kubeasz/clusters/k8s01# ezctl setup k8s01 07 

# 安装了coredns和metricserver
root@ansible:~# kubectl get svc -A
NAMESPACE     NAME                TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                  AGE
default       kubernetes          ClusterIP   10.68.0.1       <none>        443/TCP                  7m19s
kube-system   kube-dns            ClusterIP   10.68.0.2       <none>        53/UDP,53/TCP,9153/TCP   110s
kube-system   kube-dns-upstream   ClusterIP   10.68.170.64    <none>        53/UDP,53/TCP            109s
kube-system   metrics-server      ClusterIP   10.68.200.157   <none>        443/TCP                  102s
kube-system   node-local-dns      ClusterIP   None            <none>        9253/TCP                 109s
root@ansible:~# kubectl get pods -o wide -A
NAMESPACE     NAME                              READY   STATUS    RESTARTS   AGE    IP            NODE          NOMINATED NODE   READINESS GATES
kube-system   coredns-596755dbff-ngtf7          1/1     Running   0          116s   172.20.2.2    172.16.0.13   <none>           <none>
kube-system   kube-flannel-ds-n9nts             1/1     Running   0          4m5s   172.16.0.11   172.16.0.11   <none>           <none>
kube-system   kube-flannel-ds-p9kl2             1/1     Running   0          4m5s   172.16.0.12   172.16.0.12   <none>           <none>
kube-system   kube-flannel-ds-zd988             1/1     Running   0          4m5s   172.16.0.13   172.16.0.13   <none>           <none>
kube-system   metrics-server-5d648558d9-ndkm8   1/1     Running   0          108s   172.20.1.2    172.16.0.12   <none>           <none>
kube-system   node-local-dns-6x6wt              1/1     Running   0          115s   172.16.0.11   172.16.0.11   <none>           <none>
kube-system   node-local-dns-7zd8p              1/1     Running   0          115s   172.16.0.13   172.16.0.13   <none>           <none>
kube-system   node-local-dns-cch55              1/1     Running   0          115s   172.16.0.12   172.16.0.12   <none>           <none>
root@ansible:~# kubectl top nodes
NAME          CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   
172.16.0.11   46m          1%     1586Mi          21%       
172.16.0.12   26m          0%     1120Mi          15%       
172.16.0.13   25m          0%     1118Mi          15%       
```



#### 2.4 创建测试服务

```bash
root@ansible:~/k8s# cat app.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deployment
  namespace: default
spec:
  replicas: 2
  selector:
    matchLabels:
      app: myapp
      release: canary
  template:
    metadata:
      labels:
        app: myapp
        release: canary
    spec:
      containers:
      - name: myapp-container
        image: ikubernetes/myapp:v2
        imagePullPolicy: IfNotPresent
        ports:
        - name: http
          containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: myapp
  namespace: default
spec:
  selector:
    app: myapp
    release: canary
  type: NodePort
  ports:
  - port: 80
    targetPort: 80
    nodePort: 30080 #not set have defaults values.

# 安装服务
root@ansible:~/k8s# kubectl  apply -f app.yaml
root@ansible:~/k8s# kubectl  get pods -o wide 
NAME                                READY   STATUS    RESTARTS   AGE   IP           NODE          NOMINATED NODE   READINESS GATES
myapp-deployment-865b6b5b89-pm697   1/1     Running   0          71s   172.20.2.3   172.16.0.13   <none>           <none>
myapp-deployment-865b6b5b89-rflts   1/1     Running   0          71s   172.20.1.3   172.16.0.12   <none>           <none>
root@ansible:~/k8s# kubectl  get svc -o wide 
NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE   SELECTOR
kubernetes   ClusterIP   10.68.0.1      <none>        443/TCP        24m   <none>
myapp        NodePort    10.68.82.226   <none>        80:30080/TCP   79s   app=myapp,release=canary

# 测试服务
root@ansible:~/k8s# curl 172.16.0.11:30080
Hello MyApp | Version: v2 | <a href="hostname.html">Pod Name</a>
```





### 2. 部署k8s集群-kubeadm



**集群拆除**

```bash
#!/bin/bash
kubeadm reset -f
modprobe -r ipip
lsmod
rm -rf ~/.kube/
rm -rf /etc/kubernetes/
rm -rf /etc/systemd/system/kubelet.service.d
rm -rf /etc/systemd/system/kubelet.service
rm -rf /usr/bin/kube*
rm -rf /etc/cni
rm -rf /opt/cni
rm -rf /var/lib/etcd
rm -rf /var/etcd
apt-get -y remove kubeadm* kubectl* kubelet*
reboot
```





#### 2.1 安装运行时环境

**所有节点安装docker**

```bash
root@k8s-master:~# apt update && apt-get -y install apt-transport-https ca-certificates curl software-properties-common
root@k8s-master:~# curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add -
root@k8s-master:~# sudo add-apt-repository "deb [arch=amd64] https://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable"
root@k8s-master:~# sudo apt-get -y update
root@k8s-master:~# apt-cache madison docker-ce
root@k8s-master:~# apt install -y docker-ce=5:19.03.15~3-0~ubuntu-bionic

# ansible批量安装
root@ansible:~# ansible k8s -m shell -a 'sudo apt-get -y install apt-transport-https ca-certificates curl software-properties-common && curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add - && sudo add-apt-repository "deb [arch=amd64] https://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable" && sudo apt-get -y update && apt install -y docker-ce=5:19.03.15~3-0~ubuntu-bionic'
root@ansible:~# ansible k8s -m shell -a 'docker version | grep Version'
172.16.0.12 | SUCCESS | rc=0 >>
 Version:           24.0.2
  Version:          19.03.15
  Version:          1.6.21
  Version:          1.1.7
  Version:          0.18.0

172.16.0.11 | SUCCESS | rc=0 >>
 Version:           24.0.2
  Version:          19.03.15
  Version:          1.6.21
  Version:          1.1.7
  Version:          0.18.0

172.16.0.13 | SUCCESS | rc=0 >>
 Version:           24.0.2
  Version:          19.03.15
  Version:          1.6.21
  Version:          1.1.7
  Version:          0.18.0
root@ansible:~# ansible k8s -m shell -a 'systemctl is-enabled docker'
172.16.0.13 | SUCCESS | rc=0 >>
enabled

172.16.0.11 | SUCCESS | rc=0 >>
enabled

172.16.0.12 | SUCCESS | rc=0 >>
enabled

# 配置docker镜像加速
root@ansible:~/ansible/k8s/docker# cat daemon.json
{
	"exec-opts": ["native.cgroupdriver=systemd"],
	"registry-mirrors": ["https://1vjewotk.mirror.aliyuncs.com"],
	"log-driver":"json-file",
	"log-opts": {"max-size":"500m", "max-file":"3"}
}
root@ansible:~/ansible/k8s/docker# ansible k8s -m copy -a 'src=./daemon.json dest=/etc/docker/'
root@ansible:~/ansible/k8s/docker# ansible k8s -m shell -a 'systemctl restart docker'
root@ansible:~/ansible/k8s/docker# ansible k8s -m shell -a 'docker info | grep 1vjewotk.mirror.aliyuncs.com'
```



#### 2.2 配置k8s安装源

**所有节点配置**

```bash
root@k8s-master:~# apt-get update && apt-get install -y apt-transport-https
root@k8s-master:~# curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - 
root@k8s-master:~# cat <<EOF >/etc/apt/sources.list.d/kubernetes.list
deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main
EOF

# ansible配置
root@ansible:~# ansible k8s -m shell -a 'apt-get update && apt-get install -y apt-transport-https && curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - && echo "deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main" > /etc/apt/sources.list.d/kubernetes.list '
root@ansible:~# ansible k8s -m shell -a 'cat /etc/apt/sources.list.d/kubernetes.list'
172.16.0.11 | SUCCESS | rc=0 >>
deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main

172.16.0.13 | SUCCESS | rc=0 >>
deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main

172.16.0.12 | SUCCESS | rc=0 >>
deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main

```

**所有节点安装kubernetes依赖工具**

```bash
root@k8s-master:~# apt-get update
# 查看历史版本，方便选择特定版本安装
root@k8s-master:~# apt-cache madison kubelet kubeadm kubectl
# 选择特定版本安装，安装k8s1.23.16
root@k8s-master:~# apt-get install -y kubelet=1.23.16-00 kubeadm=1.23.16-00 kubectl=1.23.16-00

# ansible
root@ansible:~# ansible k8s -m shell -a 'apt-get update && apt-cache madison kubelet kubeadm kubectl && apt-get install -y kubelet=1.23.16-00 kubeadm=1.23.16-00 kubectl=1.23.16-00'
root@ansible:~# ansible k8s -m shell -a 'kubelet --version'
172.16.0.12 | SUCCESS | rc=0 >>
Kubernetes v1.23.16

172.16.0.13 | SUCCESS | rc=0 >>
Kubernetes v1.23.16

172.16.0.11 | SUCCESS | rc=0 >>
Kubernetes v1.23.16
```

**配置kubelet服务**

```bash
# 查看运行时环境docker使用的驱动，为cgroupfs
root@ansible:~# ansible k8s -m shell -a 'docker info | grep -i cgroup'
172.16.0.11 | SUCCESS | rc=0 >>
 Cgroup Driver: systemdWARNING: No swap limit support

172.16.0.12 | SUCCESS | rc=0 >>
 Cgroup Driver: systemdWARNING: No swap limit support

172.16.0.13 | SUCCESS | rc=0 >>
 Cgroup Driver: systemdWARNING: No swap limit support

# 配置kubelet使用systemd
root@ansible:~# ansible k8s -m shell -a 'echo Environment="KUBELET_CGROUP_ARGS=--cgroup-driver=systemd" >> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf && && grep KUBELET_CGROUP_ARGS /etc/systemd/system/kubelet.service.d/10-kubeadm.conf' 
172.16.0.11 | SUCCESS | rc=0 >>
Environment=KUBELET_CGROUP_ARGS=--cgroup-driver=cgroupfs

172.16.0.13 | SUCCESS | rc=0 >>
Environment=KUBELET_CGROUP_ARGS=--cgroup-driver=cgroupfs

172.16.0.12 | SUCCESS | rc=0 >>
Environment=KUBELET_CGROUP_ARGS=--cgroup-driver=cgroupfs

root@ansible:~# ansible k8s -m shell -a 'systemctl enable kubelet && systemctl start kubelet && systemctl is-active kubelet '
172.16.0.13 | SUCCESS | rc=0 >>
active

172.16.0.12 | SUCCESS | rc=0 >>
active

172.16.0.11 | SUCCESS | rc=0 >>
active
```



#### 2.3 安装第一个控制平面

```bash
# 配置nginx
root@k8s-master:~# nslookup k8s-api.jack.com
Server:		172.16.0.10
Address:	172.16.0.10#53

Name:	k8s-api.jack.com
Address: 172.16.0.14

root@ansible:/etc/nginx# cat nginx.conf 
stream {
        upstream backend {
                server 172.16.0.11:6443    max_fails=2 fail_timeout=3s;
        }
        server {
                listen 0.0.0.0:6443;
                proxy_connect_timeout 1s;
                proxy_pass backend;
        }
}


# 安装第一个master
# k8s集群安装失败时，用此命令清除
# kubeadm reset 
root@k8s-master:~# kubeadm  init \
--image-repository registry.aliyuncs.com/google_containers \
--kubernetes-version v1.23.16 \
--control-plane-endpoint k8s-api.jack.com \
--apiserver-advertise-address 172.16.0.11 \
--pod-network-cidr 10.244.0.0/16 \
--token-ttl 0 \
| tee kubeadm-init.txt

[init] Using Kubernetes version: v1.23.16
[preflight] Running pre-flight checks
	[WARNING Hostname]: hostname "k8s-master" could not be reached
	[WARNING Hostname]: hostname "k8s-master": lookup k8s-master on 172.16.0.10:53: no such host
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [k8s-api.jack.com k8s-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 172.16.0.11]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [k8s-master localhost] and IPs [172.16.0.11 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [k8s-master localhost] and IPs [172.16.0.11 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 4.879826 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.23" in namespace kube-system with the configuration for the kubelets in the cluster
NOTE: The "kubelet-config-1.23" naming of the kubelet ConfigMap is deprecated. Once the UnversionedKubeletConfigMap feature gate graduates to Beta the default name will become just "kubelet-config". Kubeadm upgrade will handle this transition transparently.
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node k8s-master as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node k8s-master as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: ctstow.2f7xwrcz11y4v4ri
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of control-plane nodes by copying certificate authorities
and service account keys on each node and then running the following as root:

  kubeadm join k8s-api.jack.com:6443 --token ctstow.2f7xwrcz11y4v4ri \
	--discovery-token-ca-cert-hash sha256:5fcc8f4dbe2400f65c6a3e343b7e7e6262539ad5e9ad002b9f6bd403b80e6950 \
	--control-plane 

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join k8s-api.jack.com:6443 --token ctstow.2f7xwrcz11y4v4ri \
	--discovery-token-ca-cert-hash sha256:5fcc8f4dbe2400f65c6a3e343b7e7e6262539ad5e9ad002b9f6bd403b80e6950 

root@k8s-master:~# kubectl get nodes
NAME         STATUS     ROLES                  AGE   VERSION
k8s-master   NotReady   control-plane,master   63m   v1.23.16

root@k8s-master:~# kubectl get pods -A
NAMESPACE      NAME                                 READY   STATUS     RESTARTS   AGE
kube-system    coredns-6d8c4cb4d-8mbj9              0/1     Pending    0          65m
kube-system    coredns-6d8c4cb4d-pg7n2              0/1     Pending    0          65m
kube-system    etcd-k8s-master                      1/1     Running    1          65m
kube-system    kube-apiserver-k8s-master            1/1     Running    1          65m
kube-system    kube-controller-manager-k8s-master   1/1     Running    1          65m
kube-system    kube-proxy-n4p5x                     1/1     Running    0          65m
kube-system    kube-scheduler-k8s-master            1/1     Running    1          65m

```

**使用kubeadm.conf初始化集群**

```bash
root@k8s-master:~# cat kubeadm-config-new.yaml
apiVersion: kubeadm.k8s.io/v1beta3
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 172.16.0.11
  bindPort: 6443
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  imagePullPolicy: IfNotPresent
  name: k8s-master
  taints: null
  kubeletExtraArgs:
    cloud-provider: external
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta3
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager: {}
dns: {}
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.aliyuncs.com/google_containers
kind: ClusterConfiguration
kubernetesVersion: 1.23.16
networking:
  dnsDomain: cluster.local
  serviceSubnet: 10.96.0.0/12
  pod-network-cidr: 10.244.0.0/16
controlPlaneEndpoint: k8s-api.jack.com
scheduler: {}
root@k8s-master:~# kubeadm init --config kubeadm-config-new.yaml
```





#### 2.4 部署网络组件

```bash
root@k8s-master:~# kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml

# 网络组件部署完成后，CoreDNS也会运行成功
root@k8s-master:~# kubectl get pods -A -o wide 
NAMESPACE      NAME                                 READY   STATUS    RESTARTS   AGE     IP            NODE         NOMINATED NODE   READINESS GATES
kube-flannel   kube-flannel-ds-mz4vp                1/1     Running   0          2m27s   172.16.0.11   k8s-master   <none>           <none>
kube-system    coredns-6d8c4cb4d-8mbj9              1/1     Running   0          67m     10.244.0.3    k8s-master   <none>           <none>
kube-system    coredns-6d8c4cb4d-pg7n2              1/1     Running   0          67m     10.244.0.2    k8s-master   <none>           <none>
kube-system    etcd-k8s-master                      1/1     Running   1          68m     172.16.0.11   k8s-master   <none>           <none>
kube-system    kube-apiserver-k8s-master            1/1     Running   1          68m     172.16.0.11   k8s-master   <none>           <none>
kube-system    kube-controller-manager-k8s-master   1/1     Running   1          67m     172.16.0.11   k8s-master   <none>           <none>
kube-system    kube-proxy-n4p5x                     1/1     Running   0          67m     172.16.0.11   k8s-master   <none>           <none>
kube-system    kube-scheduler-k8s-master            1/1     Running   1          67m     172.16.0.11   k8s-master   <none>           <none>

```



#### 2.5 部署worker节点

```bash
root@ansible:~# ansible '~172.16.0.1[23]' -m shell -a 'kubeadm join k8s-api.jack.com:6443 --token ctstow.2f7xwrcz11y4v4ri --discovery-token-ca-cert-hash sha256:5fcc8f4dbe2400f65c6a3e343b7e7e6262539ad5e9ad002b9f6bd403b80e6950'

root@k8s-master:~# kubectl get nodes
NAME           STATUS   ROLES                  AGE     VERSION
k8s-master     Ready    control-plane,master   72m     v1.23.16
k8s-worker01   Ready    <none>                 3m21s   v1.23.16
k8s-worker02   Ready    <none>                 3m21s   v1.23.16
root@k8s-master:~# kubectl  get pods -o wide -A 
NAMESPACE      NAME                                 READY   STATUS    RESTARTS   AGE     IP            NODE           NOMINATED NODE   READINESS GATES
kube-flannel   kube-flannel-ds-4gj97                1/1     Running   0          3m22s   172.16.0.12   k8s-worker01   <none>           <none>
kube-flannel   kube-flannel-ds-mz4vp                1/1     Running   0          7m22s   172.16.0.11   k8s-master     <none>           <none>
kube-flannel   kube-flannel-ds-vh6xv                1/1     Running   0          3m22s   172.16.0.13   k8s-worker02   <none>           <none>
kube-system    coredns-6d8c4cb4d-8mbj9              1/1     Running   0          72m     10.244.0.3    k8s-master     <none>           <none>
kube-system    coredns-6d8c4cb4d-pg7n2              1/1     Running   0          72m     10.244.0.2    k8s-master     <none>           <none>
kube-system    etcd-k8s-master                      1/1     Running   1          72m     172.16.0.11   k8s-master     <none>           <none>
kube-system    kube-apiserver-k8s-master            1/1     Running   1          72m     172.16.0.11   k8s-master     <none>           <none>
kube-system    kube-controller-manager-k8s-master   1/1     Running   1          72m     172.16.0.11   k8s-master     <none>           <none>
kube-system    kube-proxy-kdvn8                     1/1     Running   0          3m22s   172.16.0.12   k8s-worker01   <none>           <none>
kube-system    kube-proxy-n4p5x                     1/1     Running   0          72m     172.16.0.11   k8s-master     <none>           <none>
kube-system    kube-proxy-rsj7c                     1/1     Running   0          3m22s   172.16.0.13   k8s-worker02   <none>           <none>
kube-system    kube-scheduler-k8s-master            1/1     Running   1          72m     172.16.0.11   k8s-master     <none>           <none>

```





### 3. 部署LoadBalancer服务

**无论是kubeasz还是kubeadm方式部署的k8s集群(1.23.16、1.23.17，都未成功实现LoadBalancer功能)**



#### 3.1 配置apiserver和controller-manager

**在所有的master节点上配置**

**kubeadm**

```bash
root@k8s-master:/etc/kubernetes/manifests# grep -C1 'cloud-provider=external' /etc/kubernetes/manifests/kube-apiserver.yaml
    - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    - --cloud-provider=external
    image: registry.aliyuncs.com/google_containers/kube-apiserver:v1.23.16
root@k8s-master:/etc/kubernetes/manifests# grep -C1 'cloud-provider=external' /etc/kubernetes/manifests/kube-controller-manager.yaml 
    - --use-service-account-credentials=true
    - --cloud-provider=external
    image: registry.aliyuncs.com/google_containers/kube-controller-manager:v1.23.16

```



**kubeasz**

```bash
# 配置apiserver，添加云提供商为外部
# 添加配置前
root@k8s001:~# grep -C1 'cloud-provider=external' /etc/systemd/system/kube-apiserver.service
root@k8s001:~# vim /etc/systemd/system/kube-apiserver.service
# 添加配置后
root@k8s001:~# grep -C1 'cloud-provider=external' /etc/systemd/system/kube-apiserver.service
  --enable-aggregator-routing=true \
  --cloud-provider=external \		# 添加此行配置
  --v=2


# 配置controller-manager，添加云提供商为外部
root@k8s001:~# grep -C1 'cloud-provider=external' /etc/systemd/system/kube-controller-manager.service 
root@k8s001:~# vim /etc/systemd/system/kube-controller-manager.service
root@k8s001:~# grep -C1 'cloud-provider=external' /etc/systemd/system/kube-controller-manager.service 
  --use-service-account-credentials=true \
  --cloud-provider=external \		# 添加此行配置
  --v=2

# 重启apiserver和controller-manager服务，使配置生效
root@k8s001:~# systemctl daemon-reload 
root@k8s001:~# systemctl restart kube-apiserver.service kube-controller-manager.service 
root@k8s001:~# systemctl is-active kube-apiserver.service kube-controller-manager.service 
active
active
```



#### 3.2 在每台主机上都设置实例id与区域id

**在所有节点在配置**



**kubeadm**

```bash
---
META_EP=http://100.100.100.200/latest/meta-data
provider_id=`curl -s $META_EP/region-id`.`curl -s $META_EP/instance-id`
currente_node=`hostname`
echo "currente_node: $currente_node, provider_id: $provider_id"

kubectl patch node $currente_node -p '{"spec":{"providerID": "'$provider_id'"}}'
kubectl get nodes $currente_node -o jsonpath='{.spec.providerID}';echo
---

# master
root@k8s-master:/etc/kubernetes/manifests# META_EP=http://100.100.100.200/latest/meta-data
root@k8s-master:/etc/kubernetes/manifests# provider_id=`curl -s $META_EP/region-id`.`curl -s $META_EP/instance-id`
root@k8s-master:/etc/kubernetes/manifests# currente_node=`hostname`
root@k8s-master:/etc/kubernetes/manifests# echo "currente_node: $currente_node, provider_id: $provider_id"
currente_node: k8s-master, provider_id: cn-shanghai.i-uf6ct0ua4fcfc3ybf893
root@k8s-master:/etc/kubernetes/manifests# kubectl patch node $currente_node -p '{"spec":{"providerID": "'$provider_id'"}}'
node/k8s-master patched
root@k8s-master:/etc/kubernetes/manifests# kubectl get nodes $currente_node -o jsonpath='{.spec.providerID}';echo
cn-shanghai.i-uf6ct0ua4fcfc3ybf893

# worker01
root@k8s-worker01:~# META_EP=http://100.100.100.200/latest/meta-data
root@k8s-worker01:~# provider_id=`curl -s $META_EP/region-id`.`curl -s $META_EP/instance-id`
root@k8s-worker01:~# currente_node=`hostname`
root@k8s-worker01:~# echo "currente_node: $currente_node, provider_id: $provider_id"
currente_node: k8s-worker01, provider_id: cn-shanghai.i-uf6ct0ua4fcfc3ybf894
root@k8s-master:/etc/kubernetes/manifests# kubectl patch node k8s-worker01 -p '{"spec":{"providerID": "'cn-shanghai.i-uf6ct0ua4fcfc3ybf894'"}}'
node/k8s-worker01 patched
root@k8s-master:/etc/kubernetes/manifests# kubectl get nodes k8s-worker01 -o jsonpath='{.spec.providerID}';echo
cn-shanghai.i-uf6ct0ua4fcfc3ybf894

# worker02
root@k8s-worker02:~# META_EP=http://100.100.100.200/latest/meta-data
root@k8s-worker02:~# provider_id=`curl -s $META_EP/region-id`.`curl -s $META_EP/instance-id`
root@k8s-worker02:~# currente_node=`hostname`
root@k8s-worker02:~# echo "currente_node: $currente_node, provider_id: $provider_id"
currente_node: k8s-worker02, provider_id: cn-shanghai.i-uf6ct0ua4fcfc3ybf895
root@k8s-master:/etc/kubernetes/manifests# kubectl patch node k8s-worker02 -p '{"spec":{"providerID": "'cn-shanghai.i-uf6ct0ua4fcfc3ybf895'"}}'
node/k8s-worker02 patched
root@k8s-master:/etc/kubernetes/manifests# kubectl get nodes k8s-worker02 -o jsonpath='{.spec.providerID}';echo
cn-shanghai.i-uf6ct0ua4fcfc3ybf895
```





**kubeasz**

```bash
# 编写脚本
root@ansible:~/k8s# cat config-region-instance-id.sh 
#!/bin/bash
META_EP=http://100.100.100.200/latest/meta-data
provider_id=`curl -s $META_EP/region-id`.`curl -s $META_EP/instance-id`
currente_node_ip=`ip address show eth0 | grep 'inet ' | awk '{print $2}' | awk -F '/' '{print $1}'`
echo "currente_node_ip: $currente_node_ip, provider_id: $provider_id"

/opt/kube/bin/kubectl patch node $currente_node_ip -p '{"spec":{"providerID": "'$provider_id'"}}'
/opt/kube/bin/kubectl get nodes $currente_node_ip -o jsonpath='{.spec.providerID}';echo
---

# 分布脚本到所有k8s节点
root@ansible:~/k8s# ansible k8s -m copy -a 'src=./config-region-instance-id.sh dest=/tmp/ mode=744'
root@ansible:~/k8s# ansible k8s -m shell -a 'ls -l /tmp/config-region-instance-id.sh'
172.16.0.11 | SUCCESS | rc=0 >>
-rwxr--r-- 1 root root 461 Mar 10 17:14 /tmp/config-region-instance-id.sh

172.16.0.12 | SUCCESS | rc=0 >>
-rwxr--r-- 1 root root 461 Mar 10 17:14 /tmp/config-region-instance-id.sh

172.16.0.13 | SUCCESS | rc=0 >>
-rwxr--r-- 1 root root 461 Mar 10 17:14 /tmp/config-region-instance-id.sh

# 运行脚本配置所有k8s节点的实例id与区域id
root@ansible:~/k8s# ansible k8s -m shell -a '/tmp/config-region-instance-id.sh'
172.16.0.13 | SUCCESS | rc=0 >>
currente_node_ip: 172.16.0.13, provider_id: cn-shanghai.i-uf6ct0ua4fcfc3ybf895
node/172.16.0.13 patched
cn-shanghai.i-uf6ct0ua4fcfc3ybf895

172.16.0.11 | SUCCESS | rc=0 >>
currente_node_ip: 172.16.0.11, provider_id: cn-shanghai.i-uf6ct0ua4fcfc3ybf893
node/172.16.0.11 patched
cn-shanghai.i-uf6ct0ua4fcfc3ybf893

172.16.0.12 | SUCCESS | rc=0 >>
currente_node_ip: 172.16.0.12, provider_id: cn-shanghai.i-uf6ct0ua4fcfc3ybf894
node/172.16.0.12 patched
cn-shanghai.i-uf6ct0ua4fcfc3ybf894
```



#### 3.3 创建有SLB权限的API账号

1. 创建仅有API权限的子账号，不能登录阿里云

2. 存储好生成的AK、SK

3. 配置此账号权限（搜索关键字：slb）：

   * 管理负载均衡服务（SLB）的权限：AliyunSLBFullAccess

   

#### 3.4 将创建的slb权限账号的AK、SK配置为ConfigMap

**创建ConfigMap**

**kubeadm**

```bash
root@k8s-master:~/cloud-controller-manager# cat cloud-config.yaml 
apiVersion: v1
kind: ConfigMap
metadata:
  name: cloud-config
  namespace: kube-system
data:
  cloud-config.conf: |-
    {
        "Global": {
            "accessKeyID": "AK",
            "accessKeySecret": "SK"
        }
    }
root@k8s-master:~/cloud-controller-manager# kubectl apply -f cloud-config.yaml
configmap/cloud-config created
root@k8s-master:~/cloud-controller-manager# kubectl get cm cloud-config -n kube-system 
NAME           DATA   AGE
cloud-config   1      80s

```



**kubeasz**

```bash
root@ansible:~/k8s/cloud-controller-manager# cat cloud-config.yaml 
apiVersion: v1
kind: ConfigMap
metadata:
  name: cloud-config
  namespace: kube-system
data:
  cloud-config.conf: |-
    {
        "Global": {
            "accessKeyID": "AK",
            "accessKeySecret": "SK"
        }
    }
root@ansible:~/k8s/cloud-controller-manager# kubectl apply -f cloud-config.yaml
configmap/cloud-config created
```



#### 3.5 创建阿里云控制

**获取CA证书并base64编码**



**kubeadm**

```bash
root@k8s-master:~/cloud-controller-manager# cat /etc/kubernetes/pki/ca.crt | base64 -w 0;echo
LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUMvakNDQWVhZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJME1ETXhNekEwTURJd09Gb1hEVE0wTURNeE1UQTBNREl3T0Zvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTUxrCm5XenltSTFMVWxKeE1NYWdkQ3JrbUJKbG1RQlpVUEZsenp2WnJjbXJYbXNCLzFzWTUyeXJ2UDcxZThQeUxHVnQKODM5VXhDSVVSRlduYjgzTzJvdHdYd2MxbnhxM2wwMklEb2g5UU5PY0pqSDdrcy93MnlQT09oakJiemx3Q0xwQgpsUHVkanVXWVV5R2pscnZnS09iNFNPbmRLSjZkV2VOd0xMYXU0aE5panZrNk0wTXpNdEpqM3dqTEptUW0wYyt6CnBqSVRwUmk4cjRUU3BsL2hNZzVoUmpscTBYc2pqTHJNOTM5dUxOY1ZWRDRTZFY4d3c5RkVRSVpHVk9mOWVCa1UKNktERkczUGpoNmEyVncxTlM4WUphYk91OXVwMGwyZU1RaDZTSzNEd2pFdithR1RWa0twbXVacjFneGZHUEw4SwpHUXp0TWpsRzZqcWpGTVpkTHRrQ0F3RUFBYU5aTUZjd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZBTUJra3E0M3JreVlCL2tWT2xiV0ZoZWVkS0hNQlVHQTFVZEVRUU8KTUF5Q0NtdDFZbVZ5Ym1WMFpYTXdEUVlKS29aSWh2Y05BUUVMQlFBRGdnRUJBSWRHRUYzVElLZXNEV3ZmQ2ZndAphSVZYaG9nemJ4czhaQy9YUGhkdGtQZ0UwUkhtSCtxUDN0QmUraTdYTGtjUXZ5a0JjeEtGcVRrbTl0c2liOXo3CkF3bWFsUTJ5cksraWpySFA4RmwwbFUrL3REQVgydVpFRlk2Zmc4RmlaTWZhMFZ3OE9EdU5WYzZBbzJkNFR4ZlYKcHFHRk9LYkY5bzBrd0c3aXlyMy91eS9qc1NxbHRZV1h5TUtLUHMxVTRJdmkrSllxdnlHMnY2a1FLK0ZObFcyVAoyYTFXUUNoWlEvV0kzMWdOaUJVSG5MZ0wwNmo4VE5SRkl1QUx4cUFpZytPMVo0NmR2cnA2SEtYL01kYmduQTNyCk0zQnljbnlVNllheklsREsvRnZ3MDJVTmFQNU5RcWZHcUFwT3A2czNKUUcrSENFaVV3ZzFheGhwdWIrSnVFVkEKTVE0PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
```

**kubeasz**

```bash
root@k8s001:~# cat /etc/kubernetes/ssl/ca.pem | base64 -w 0;echo
LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURsRENDQW55Z0F3SUJBZ0lVZkxtMWRvLzEvWUs0ZjlFTVJMc2o4S2IvS1Y0d0RRWUpLb1pJaHZjTkFRRUwKQlFBd1lURUxNQWtHQTFVRUJoTUNRMDR4RVRBUEJnTlZCQWdUQ0VoaGJtZGFhRzkxTVFzd0NRWURWUVFIRXdKWQpVekVNTUFvR0ExVUVDaE1EYXpoek1ROHdEUVlEVlFRTEV3WlRlWE4wWlcweEV6QVJCZ05WQkFNVENtdDFZbVZ5CmJtVjBaWE13SUJjTk1qUXdNekE1TVRJek9UQXdXaGdQTWpFeU5EQXlNVFF4TWpNNU1EQmFNR0V4Q3pBSkJnTlYKQkFZVEFrTk9NUkV3RHdZRFZRUUlFd2hJWVc1bldtaHZkVEVMTUFrR0ExVUVCeE1DV0ZNeEREQUtCZ05WQkFvVApBMnM0Y3pFUE1BMEdBMVVFQ3hNR1UzbHpkR1Z0TVJNd0VRWURWUVFERXdwcmRXSmxjbTVsZEdWek1JSUJJakFOCkJna3Foa2lHOXcwQkFRRUZBQU9DQVE4QU1JSUJDZ0tDQVFFQTNBU2xsY0lXck1sOEtIOFV1S2RWMHhubERGeTUKRnhTOS8zMVROUERheERWNnVUUlo1eTJPSzFINGd2d2FiU2Zhbk5iTklJUlRuNDFXL3FPV1BYZ2YrUWdBK0pzaAo0VExCVzA4YS9yK0lSdEI4RTNMcytIbFJYTkpRc1ltZEtuR0dNem5mVGJoVlM1RXBGZi9ENE5pZDRsVm5VTWFaCjdXSS9LWmZ1TTN2M1REeXZPUGxmQ1Z1Y3ZuTFdaTXdKVHArYzg5S0dRbWMzeEY5WThla2lFU2VGOWJEUDFnY2MKWEdyYU9HNHhvQUx0UmhoUzBFTDBEOXpvQjlUdTNVaUFXYlhJckxJbWF2aGlFZ1BvZEtmUG9Na0tURVhJQk5lUQphdUN1OHo5emtDdFVDankvSSt4Z3FWSTZsWGQ0Y3BFZU56QldseGFncVJmL24rM1pPN2huR1A4UVBRSURBUUFCCm8wSXdRREFPQmdOVkhROEJBZjhFQkFNQ0FRWXdEd1lEVlIwVEFRSC9CQVV3QXdFQi96QWRCZ05WSFE0RUZnUVUKSkpXQmtVVGtQVVR0dXZLc21XblZEWUVZWXZVd0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFEdjlZeUVtQ05CbwoweTJPRlg4WmUvRzNpWGM5NDZjZE9QRHhlRFpja1VEWjFVU1FlZEUrNkM4Z1gzNjMrMHlXc0k2bFk1Z1d5Y1BsCjZMVWNxWmM2clVRNW5WYXlPSCtKc21LNEVzbmVzdFRteWJrbkdiTjVoWThBNlBwTnNqR1hFcis5SDBNUHU2QWkKZ0FHb0xocWlSZGJBdVliZXppMjY1aDh5eHJQNDFDblJQWGZ6S2dmT0lwRUkxMGV2dWMzM0ZudmFzMzhJUjZ4NgpJWDAzV21jdlNSZjRFU2RUeWZFQmJKM1JFTjZmOTRGZHRCdmlNZmtYWUZlTWNKMjZ5WGFYQzI1ejdUVHNOUjIzCktWOUNSL2s4UjN3cktuZ2lZMnNMVE5TR0pjenlzR0xIRXp6VGY1Q3d4Mm84NndrMTlUdzhja1hPK25DZEhRZE8KL0VaenpVY280STg9Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
```



**创建/etc/kubernetes/cloud-controller-manager.conf**

在所有master节点在创建

**kubeadm**

```bash
root@k8s-master:~/cloud-controller-manager# cat /etc/kubernetes/cloud-controller-manager.conf 
kind: Config
contexts:
- context:
    cluster: kubernetes
    user: system:cloud-controller-manager
  name: system:cloud-controller-manager@kubernetes
current-context: system:cloud-controller-manager@kubernetes
users:
- name: system:cloud-controller-manager
  user:
    tokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUMvakNDQWVhZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJME1ETXhNekEwTURJd09Gb1hEVE0wTURNeE1UQTBNREl3T0Zvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTUxrCm5XenltSTFMVWxKeE1NYWdkQ3JrbUJKbG1RQlpVUEZsenp2WnJjbXJYbXNCLzFzWTUyeXJ2UDcxZThQeUxHVnQKODM5VXhDSVVSRlduYjgzTzJvdHdYd2MxbnhxM2wwMklEb2g5UU5PY0pqSDdrcy93MnlQT09oakJiemx3Q0xwQgpsUHVkanVXWVV5R2pscnZnS09iNFNPbmRLSjZkV2VOd0xMYXU0aE5panZrNk0wTXpNdEpqM3dqTEptUW0wYyt6CnBqSVRwUmk4cjRUU3BsL2hNZzVoUmpscTBYc2pqTHJNOTM5dUxOY1ZWRDRTZFY4d3c5RkVRSVpHVk9mOWVCa1UKNktERkczUGpoNmEyVncxTlM4WUphYk91OXVwMGwyZU1RaDZTSzNEd2pFdithR1RWa0twbXVacjFneGZHUEw4SwpHUXp0TWpsRzZqcWpGTVpkTHRrQ0F3RUFBYU5aTUZjd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZBTUJra3E0M3JreVlCL2tWT2xiV0ZoZWVkS0hNQlVHQTFVZEVRUU8KTUF5Q0NtdDFZbVZ5Ym1WMFpYTXdEUVlKS29aSWh2Y05BUUVMQlFBRGdnRUJBSWRHRUYzVElLZXNEV3ZmQ2ZndAphSVZYaG9nemJ4czhaQy9YUGhkdGtQZ0UwUkhtSCtxUDN0QmUraTdYTGtjUXZ5a0JjeEtGcVRrbTl0c2liOXo3CkF3bWFsUTJ5cksraWpySFA4RmwwbFUrL3REQVgydVpFRlk2Zmc4RmlaTWZhMFZ3OE9EdU5WYzZBbzJkNFR4ZlYKcHFHRk9LYkY5bzBrd0c3aXlyMy91eS9qc1NxbHRZV1h5TUtLUHMxVTRJdmkrSllxdnlHMnY2a1FLK0ZObFcyVAoyYTFXUUNoWlEvV0kzMWdOaUJVSG5MZ0wwNmo4VE5SRkl1QUx4cUFpZytPMVo0NmR2cnA2SEtYL01kYmduQTNyCk0zQnljbnlVNllheklsREsvRnZ3MDJVTmFQNU5RcWZHcUFwT3A2czNKUUcrSENFaVV3ZzFheGhwdWIrSnVFVkEKTVE0PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    server: https://k8s-api.jack.com:6443
  name: kubernetes
```

**kubeasz**

```bash
root@k8s001:~# cat /etc/kubernetes/cloud-controller-manager.conf
kind: Config
contexts:
- context:
    cluster: kubernetes
    user: system:cloud-controller-manager
  name: system:cloud-controller-manager@kubernetes
current-context: system:cloud-controller-manager@kubernetes
users:
- name: system:cloud-controller-manager
  user:
    tokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURsRENDQW55Z0F3SUJBZ0lVZkxtMWRvLzEvWUs0ZjlFTVJMc2o4S2IvS1Y0d0RRWUpLb1pJaHZjTkFRRUwKQlFBd1lURUxNQWtHQTFVRUJoTUNRMDR4RVRBUEJnTlZCQWdUQ0VoaGJtZGFhRzkxTVFzd0NRWURWUVFIRXdKWQpVekVNTUFvR0ExVUVDaE1EYXpoek1ROHdEUVlEVlFRTEV3WlRlWE4wWlcweEV6QVJCZ05WQkFNVENtdDFZbVZ5CmJtVjBaWE13SUJjTk1qUXdNekE1TVRJek9UQXdXaGdQTWpFeU5EQXlNVFF4TWpNNU1EQmFNR0V4Q3pBSkJnTlYKQkFZVEFrTk9NUkV3RHdZRFZRUUlFd2hJWVc1bldtaHZkVEVMTUFrR0ExVUVCeE1DV0ZNeEREQUtCZ05WQkFvVApBMnM0Y3pFUE1BMEdBMVVFQ3hNR1UzbHpkR1Z0TVJNd0VRWURWUVFERXdwcmRXSmxjbTVsZEdWek1JSUJJakFOCkJna3Foa2lHOXcwQkFRRUZBQU9DQVE4QU1JSUJDZ0tDQVFFQTNBU2xsY0lXck1sOEtIOFV1S2RWMHhubERGeTUKRnhTOS8zMVROUERheERWNnVUUlo1eTJPSzFINGd2d2FiU2Zhbk5iTklJUlRuNDFXL3FPV1BYZ2YrUWdBK0pzaAo0VExCVzA4YS9yK0lSdEI4RTNMcytIbFJYTkpRc1ltZEtuR0dNem5mVGJoVlM1RXBGZi9ENE5pZDRsVm5VTWFaCjdXSS9LWmZ1TTN2M1REeXZPUGxmQ1Z1Y3ZuTFdaTXdKVHArYzg5S0dRbWMzeEY5WThla2lFU2VGOWJEUDFnY2MKWEdyYU9HNHhvQUx0UmhoUzBFTDBEOXpvQjlUdTNVaUFXYlhJckxJbWF2aGlFZ1BvZEtmUG9Na0tURVhJQk5lUQphdUN1OHo5emtDdFVDankvSSt4Z3FWSTZsWGQ0Y3BFZU56QldseGFncVJmL24rM1pPN2huR1A4UVBRSURBUUFCCm8wSXdRREFPQmdOVkhROEJBZjhFQkFNQ0FRWXdEd1lEVlIwVEFRSC9CQVV3QXdFQi96QWRCZ05WSFE0RUZnUVUKSkpXQmtVVGtQVVR0dXZLc21XblZEWUVZWXZVd0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFEdjlZeUVtQ05CbwoweTJPRlg4WmUvRzNpWGM5NDZjZE9QRHhlRFpja1VEWjFVU1FlZEUrNkM4Z1gzNjMrMHlXc0k2bFk1Z1d5Y1BsCjZMVWNxWmM2clVRNW5WYXlPSCtKc21LNEVzbmVzdFRteWJrbkdiTjVoWThBNlBwTnNqR1hFcis5SDBNUHU2QWkKZ0FHb0xocWlSZGJBdVliZXppMjY1aDh5eHJQNDFDblJQWGZ6S2dmT0lwRUkxMGV2dWMzM0ZudmFzMzhJUjZ4NgpJWDAzV21jdlNSZjRFU2RUeWZFQmJKM1JFTjZmOTRGZHRCdmlNZmtYWUZlTWNKMjZ5WGFYQzI1ejdUVHNOUjIzCktWOUNSL2s4UjN3cktuZ2lZMnNMVE5TR0pjenlzR0xIRXp6VGY1Q3d4Mm84NndrMTlUdzhja1hPK25DZEhRZE8KL0VaenpVY280STg9Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
    server: https://172.16.0.11:6443
  name: kubernetes
```



#### 3.6 安装CloudProvider daemonset



**kubeadm**

```bash
root@k8s-master:~/cloud-controller-manager# wget https://raw.githubusercontent.com/kubernetes/cloud-provider-alibaba-cloud/master/docs/examples/cloud-controller-manager.yml
root@k8s-master:~/cloud-controller-manager# SVCRANGE=$(echo '{"apiVersion":"v1","kind":"Service","metadata":{"name":"tst"},"spec":{"clusterIP":"1.1.1.1","ports":[{"port":443}]}}' | kubectl apply -f - 2>&1 | sed 's/.*valid IPs is //')
root@k8s-master:~/cloud-controller-manager# echo $SVCRANGE
10.96.0.0/12
root@k8s-master:~/cloud-controller-manager# sed -i 's#cluster-cidr=${CLUSTER_CIDR}#cluster-cidr=10.96.0.0/12#g' cloud-controller-manager.yml
root@k8s-master:~/cloud-controller-manager# grep 'cluster-cidr' cloud-controller-manager.yml
          #- --cluster-cidr=172.16.0.0/16
          - --cluster-cidr=10.96.0.0/12
root@k8s-master:~/cloud-controller-manager# sed -i 's#${ImageVersion}#v2.7.0#g' cloud-controller-manager.yml
root@k8s-master:~/cloud-controller-manager# grep 'v2.7.0' cloud-controller-manager.yml
          image: registry.cn-hangzhou.aliyuncs.com/acs/cloud-controller-manager-amd64:v2.7.0
          
# 修改标签选择器
root@k8s-master:~/cloud-controller-manager# cat cloud-controller-manager.yml |grep -A1 nodeSelector
      nodeSelector:
        node-role.kubernetes.io/master: ""
root@k8s-master:~/cloud-controller-manager# sed -i 's#node-role.kubernetes.io/master: ""#cloud-controller-manager: "true"#g' cloud-controller-manager.yml
root@k8s-master:~/cloud-controller-manager# cat cloud-controller-manager.yml |grep -A1 nodeSelector
      nodeSelector:
        cloud-controller-manager: "true"
# 在有/etc/kubernetes/cloud-controller-manager.conf文件的服务器上打上对应的标签
root@k8s-master:~/cloud-controller-manager# kubectl label node k8s-master cloud-controller-manager="true"
node/k8s-master labeled

root@k8s-master:~/cloud-controller-manager# kubectl apply -f cloud-controller-manager.yml
root@k8s-master:~/cloud-controller-manager# kubectl apply -f cloud-config.yaml

```





**kubeasz**

```bash
# 下载配置清单
root@ansible:~/k8s/cloud-controller-manager# wget https://raw.githubusercontent.com/kubernetes/cloud-provider-alibaba-cloud/master/docs/examples/cloud-controller-manager.yml

# 查看集群中service的cidr配置
root@ansible:~/k8s/cloud-controller-manager# SVCRANGE=$(echo '{"apiVersion":"v1","kind":"Service","metadata":{"name":"tst"},"spec":{"clusterIP":"1.1.1.1","ports":[{"port":443}]}}' | kubectl apply -f - 2>&1 | sed 's/.*valid IPs is //')
root@ansible:~/k8s/cloud-controller-manager# echo $SVCRANGE
10.68.0.0/16

# 修改cloud-controller-manager.yml文件中的--service-cidr地址
root@ansible:~/k8s/cloud-controller-manager# grep 'cluster-cidr' cloud-controller-manager.yml
          #- --cluster-cidr=172.16.0.0/16
          - --cluster-cidr=${CLUSTER_CIDR}
# 开始更改
root@ansible:~/k8s/cloud-controller-manager# sed -i 's#cluster-cidr=${CLUSTER_CIDR}#cluster-cidr=10.68.0.0/16#g' cloud-controller-manager.yml
root@ansible:~/k8s/cloud-controller-manager# grep 'cluster-cidr' cloud-controller-manager.yml
          #- --cluster-cidr=172.16.0.0/16
          - --cluster-cidr=10.68.0.0/16

# 修改镜像版本，镜像版本高了低了都会异常。
# 镜像版本信息:https://link.juejin.cn/?target=https%3A%2F%2Fhelp.aliyun.com%2Fzh%2Fack%2Fproduct-overview%2Fcloud-controller-manager
# k8s集群为1.23.17，于2023年3月1日打的tag，找到cloud-controller-manager为2023年6月的镜像版本：v2.7.0
root@ansible:~/k8s/cloud-controller-manager# grep 'ImageVersion' cloud-controller-manager.yml
          image: registry.cn-hangzhou.aliyuncs.com/acs/cloud-controller-manager-amd64:${ImageVersion}
root@ansible:~/k8s/cloud-controller-manager# sed -i 's#${ImageVersion}#v2.7.0#g' cloud-controller-manager.yml
root@ansible:~/k8s/cloud-controller-manager# grep 'v2.7.0' cloud-controller-manager.yml
          image: registry.cn-hangzhou.aliyuncs.com/acs/cloud-controller-manager-amd64:v2.7.0


# 修改标签选择器
root@ansible:~/k8s/cloud-controller-manager# cat cloud-controller-manager.yml |grep -A1 nodeSelector
      nodeSelector:
        node-role.kubernetes.io/master: ""
root@ansible:~/k8s/cloud-controller-manager# sed -i 's#node-role.kubernetes.io/master: ""#cloud-controller-manager: "true"#g' cloud-controller-manager.yml
root@ansible:~/k8s/cloud-controller-manager# cat cloud-controller-manager.yml |grep -A1 nodeSelector
      nodeSelector:
        cloud-controller-manager: "true"

# 在有/etc/kubernetes/cloud-controller-manager.conf文件的服务器上打上对应的标签
root@ansible:~/k8s/cloud-controller-manager# kubectl label node 172.16.0.11 cloud-controller-manager="true"
node/172.16.0.11 labeled

# 应用配置清单cloud-controller-manager.yml
root@ansible:~/k8s/cloud-controller-manager# kubectl apply -f cloud-controller-manager.yml 
clusterrole.rbac.authorization.k8s.io/system:cloud-controller-manager created
serviceaccount/cloud-controller-manager created
clusterrolebinding.rbac.authorization.k8s.io/system:cloud-controller-manager created
configmap/cloud-config configured
Warning: spec.template.metadata.annotations[scheduler.alpha.kubernetes.io/critical-pod]: non-functional in v1.16+; use the "priorityClassName" field instead
daemonset.apps/cloud-controller-manager created
# 再应用之前的cloud-config.yaml配置，因为已经被覆盖
root@ansible:~/k8s/cloud-controller-manager# kubectl apply -f cloud-config.yaml
configmap/cloud-config configured
# 查看运行的pod
root@ansible:~/k8s/cloud-controller-manager# kubectl get ds,pod -n kube-system|grep cloud-controller-manager
daemonset.apps/cloud-controller-manager   1         1         1       1            1           cloud-controller-manager=true   76s
pod/cloud-controller-manager-nbzcw    1/1     Running   0          76s
```



#### 3.7 创建测试slb

![](./image/aliyun-slb/slb01.jpg)



**创建测试的nginx**

```bash
root@ansible:~/k8s/cloud-controller-manager# cat example-app-deploy.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: example-app
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: example-app
  template:
    metadata:
      labels:
        app: example-app
    spec:
      containers:
      - name: example-app
        image: nginx:alpine
        ports:
        - name: web
          containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    service.beta.kubernetes.io/alicloud-loadbalancer-id: lb-uf655bmg4i42lzsc26l3c  #slb实例id
  name: slb
  namespace: default
spec:
  selector:
    app: example-app
  ports:
  - name: web
    port: 80
    protocol: TCP
  type: LoadBalancer
---
```

**阿里云测试未成功**







## 高可用虚拟IP（HaVip）

借助VPC提供的高可用虚拟IP HaVip（High-Availability Virtual IP Address）功能，您可在云上基于ARP协议，通过Keepalived或Heartbeat软件来搭建服务高可用架构，以确保主备切换过程中服务IP不变（即IP漂移）。





###  1. 什么是高可用虚拟IP（HaVip）

**定义**

HaVip是一种可以独立创建和释放的私网IP资源，具备与ECS实例主私网IP地址一样的网络接入能力，可以与高可用软件，例如[Keepalived](https://keepalived.readthedocs.io/en/latest/introduction.html)配合使用，搭建高可用主备服务，提高业务的可用性。

HaVip支持绑定一个弹性公网IP（EIP）、多个ECS实例或多个ECS实例的主网卡或辅助网卡，以实现同可用区、多服务器高可用架构下的IP漂移，确保对外提供服务的私网IP始终不变。此外，该架构下的多个ECS实例还可以利用部署集能力进一步提升业务的可靠性



**Keepalived就可以支持实现虚拟IP高可用，为什么要配合HaVip来实现呢？**

在传统数据中心里，服务器可以通过地址解析协议ARP（Address Resolution Protocol）声明自己的IP地址并对外提供服务，很多应用场景或常用软件需要主机具备此能力。例如，使用Keepalived、Heartbeat等软件实现容灾恢复过程中服务IP不变的高可用方案。

然而，大部分云厂商采用SDN架构后，在VPC环境下不支持免费ARP广播功能。因为云上网络环境需使用虚拟化技术构建，虚拟服务器IP地址由云平台底层的虚拟化平台分配和管理。您的应用无法像传统方式一样修改主机IP地址，且整个虚拟网络是基于3层的隧道技术，ARP被终结在发送端，主机无法声明IP地址。为此，阿里云推出HaVip功能，解决此问题。



### 2. 使用场景

**场景一：面向公网的高可用服务**

* ECS1和ECS2实例通过Keepalived实现主备高可用，并与HaVip成功绑定；
* 其中，ECS1实例通过ARP宣告该HaVip。宣告成功后，ECS1作为主实例通过与HaVip绑定的EIP对外提供服务，ECS2作为备用ECS实例。
* 当ECS1发生故障时，ECS2会自动调用自身的接管程序，接管ECS1的服务，实现业务高可用。



**场景二：面向私网的高可用服务**

* ECS1和ECS2实例基于HaVip，使用Keepalived组合成一个高可用的私网服务。
* VPC内的其他实例ECS3可以通过私网访问该服务，服务地址为HaVip的地址。
* 当ECS1发生故障时，ECS2会自动调用自身的接管程序，接管ECS1的服务，实现业务高可用。



**配额和费用**

HaVip功能正在公测，您可以登录阿里云[配额中心控制台](https://quotas.console.aliyun.com/products/vpc/quotas?query=vpc_privilege_allow_buy_havip_instance)进行自助申请。

> **重要**  公测期间，HaVip免费使用，且不承诺任何服务等级协议（SLA）相关的保障条款。



**使用限制与配额**

* 支持创建高可用虚拟IP（HaVip）的网络类型: VPC类型

* 单个ECS实例支持同时绑定的HaVip数量: 5个

* 单个HaVip支持同时绑定的EIP数量: 1个

* 单个HaVip支持同时绑定的ECS实例或弹性网卡的数量: 10个

* HaVip是否支持广播和组播通信: 不支持

  > HaVip只支持单播，如果您使用Keepalived等第三方软件实现高可用，需要修改配置文件中的通信方式为单播通信。

* 单个账号支持创建的HaVip的数量、单个VPC支持创建的HaVip的数量: 50个





### 3. HaVip使用

![](../image/aliyun/HaVip/01.png)



#### 3.1 创建HaVip



### 3.2 安装nginx

```bash
# server01
[root@server01 ~]# yum install -y nginx
[root@server01 ~]# curl localhost
my is keepalived server01-172.16.0.1

# server02
[root@server02 ~]# yum install -y nginx
[root@server02 ~]# curl localhost
my is keepalived server02-172.16.0.2
```





### 3.3 在主备ECS实例部署keepalived



**环境**

| 服务器名称 | IP地址     | 角色                     | HaVip        |
| ---------- | ---------- | ------------------------ | ------------ |
| server01   | 172.16.0.1 | keepalived master、nginx | 172.16.0.100 |
| server02   | 172.16.0.2 | keepalived backup、nginx | 172.16.0.100 |
| server03   | 172.16.0.3 | client                   |              |



**安装keepalived**

172.16.0.1和172.16.0.2安装keepalived

```bash
[root@server01 ~]# yum install -y keepalived
[root@server01 ~]# cp /etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf.bak
[root@server02 ~]# yum install -y keepalived
[root@server02 ~]# cp /etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf.bak
```

**配置keepalived**

master

```bash
[root@server01 ~]# vim /etc/keepalived/keepalived.conf
! Configuration File for keepalived
global_defs {
	notification_email {
     		root@localhost
   	}
   	notification_email_from root@localhost
   	smtp_server 127.0.0.1
   	smtp_connect_timeout 30
   	router_id nginx_ha01
}

vrrp_script chk_nginx {              
    	script "/etc/keepalived/chk_nginx.sh"
    	interval 1
    	weight 10 
}

vrrp_instance nginx_ha {
	state MASTER
	interface eth0
	virtual_router_id 80
	priority 120
	advert_int 1
   	unicast_src_ip 172.16.0.1
	unicast_peer {              
    	172.16.0.2
    }

	authentication {
		auth_type PASS
       	auth_pass 8486c8cdb3 
	}

	virtual_ipaddress {
		172.16.0.100
	}

	track_script {
    	chk_nginx
    }
	
	notify_master "/etc/keepalived/notify.sh master"  
	notify_backup "/etc/keepalived/notify.sh backup"  
	notify_fault "/etc/keepalived/notify.sh fault"  
	smtp alter
}
---
# chk_nginx.sh
#!/bin/bash
d=`date --date today +%Y%m%d_%H:%M:%S`
/usr/bin/netstat -tnlp | /usr/bin/grep :80 >& /dev/null
if [ $? -ne '0' ];then
    /usr/bin/systemctl stop nginx
	/usr/bin/systemctl start nginx
	/usr/bin/netstat -tnlp | /usr/bin/grep :80 >& /dev/null
    if [ $? -ne "0"  ]; then
    	echo "$d: nginx is down,keepalived will stop" >> /var/log/keepalived_chk_nginx.log
        /usr/bin/systemctl stop keepalived
     fi
fi
---
# notify.sh
#!/bin/bash
#
contact='notify@test.com'
INTERFACE_NAME=`grep interface /etc/keepalived/keepalived.conf | awk '{print $2}'`
INTERFACE_NAME_IPADDR=`ip a s ${INTERFACE_NAME} | grep 172.16.0.255 | awk -F '/' '{print $1}' | awk '{print $2}'`
notify() {
local mailsubject="$(hostname) to be $1, vip floating"
local mailbody="$(date +'%F %T'): vrrp transition, $(hostname)-${INTERFACE_NAME_IPADDR} changed to be $1"
echo "$mailbody" | mail -s "$mailsubject" $contact
}
  
case $1 in
master)
        notify master
        ;;
backup)
        notify backup
        ;;
fault)
        notify fault
        ;;
*)
        echo "Usage: $(basename $0) {master|backup|fault}"
        exit 1
        ;;
esac
```

backup

```bash
[root@server02 ~]# vim /etc/keepalived/keepalived.conf
! Configuration File for keepalived
global_defs {
	notification_email {
     		root@localhost
   	}
   	notification_email_from root@localhost
   	smtp_server 127.0.0.1
   	smtp_connect_timeout 30
   	router_id nginx_ha02
}

vrrp_script chk_nginx {              
    	script "/etc/keepalived/chk_nginx.sh"
    	interval 1
    	weight 10 
}

vrrp_instance nginx_ha {
	state MASTER
	interface eth0
	virtual_router_id 80
	priority 100
	advert_int 1
   	unicast_src_ip 172.16.0.2
	unicast_peer {              
    	172.16.0.1
    }

	authentication {
		auth_type PASS
       	auth_pass 8486c8cdb3 
	}

	virtual_ipaddress {
		172.16.0.100
	}

	track_script {
    	chk_nginx
    }
	
	notify_master "/etc/keepalived/notify.sh master"  
	notify_backup "/etc/keepalived/notify.sh backup"  
	notify_fault "/etc/keepalived/notify.sh fault"  
	smtp alter
}
---
# chk_nginx.sh
#!/bin/bash
d=`date --date today +%Y%m%d_%H:%M:%S`
/usr/bin/netstat -tnlp | /usr/bin/grep :80 >& /dev/null
if [ $? -ne '0' ];then
    /usr/bin/systemctl stop nginx
	/usr/bin/systemctl start nginx
	/usr/bin/netstat -tnlp | /usr/bin/grep :80 >& /dev/null
    if [ $? -ne "0"  ]; then
    	echo "$d: nginx is down,keepalived will stop" >> /var/log/keepalived_chk_nginx.log
        /usr/bin/systemctl stop keepalived
     fi
fi
---
# notify.sh
#!/bin/bash
#
contact='notify@test.com'
INTERFACE_NAME=`grep interface /etc/keepalived/keepalived.conf | awk '{print $2}'`
INTERFACE_NAME_IPADDR=`ip a s ${INTERFACE_NAME} | grep 172.16.0.255 | awk -F '/' '{print $1}' | awk '{print $2}'`
notify() {
local mailsubject="$(hostname) to be $1, vip floating"
local mailbody="$(date +'%F %T'): vrrp transition, $(hostname)-${INTERFACE_NAME_IPADDR} changed to be $1"
echo "$mailbody" | mail -s "$mailsubject" $contact
}
  
case $1 in
master)
        notify master
        ;;
backup)
        notify backup
        ;;
fault)
        notify fault
        ;;
*)
        echo "Usage: $(basename $0) {master|backup|fault}"
        exit 1
        ;;
esac
```



**启动keepalived**

```bash
[root@server01 ~]# systemctl enable keepalived
[root@server01 ~]# systemctl start keepalived
[root@server02 ~]# systemctl enable keepalived
[root@server02 ~]# systemctl start keepalived
```



**keepalived ECS绑定HaVip**

将HaVip分别与主备ECS实例绑定。

1. 登录[专有网络管理控制台](https://vpcnext.console.aliyun.com/vpc)。
2. 在左侧导航栏，单击**高可用虚拟IP**。
3. 在顶部菜单栏处，选择HaVip的地域。
4. 找到[步骤一：创建HaVip](https://help.aliyun.com/zh/vpc/implement-high-availability-by-using-havips-and-keepalived#section-poa-9bp-9lt)创建的HaVip实例，单击HaVip实例的ID。
5. 在**绑定资源**区域**ECS实例**处，单击**立即绑定**。



**测试**

```bash
[root@server01 ~]# ip a s 
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host 
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000
    link/ether 1a:12:ea:20:97:ec brd ff:ff:ff:ff:ff:ff
    inet 172.16.0.1/24 brd 172.16.0.255 scope global eth0
       valid_lft forever preferred_lft forever
    inet 172.16.0.100/32 scope global eth0
       valid_lft forever preferred_lft forever

[root@server02 ~]# ip a s 
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host 
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000
    link/ether 1a:12:ea:20:97:ec brd ff:ff:ff:ff:ff:ff
    inet 172.16.0.2/24 brd 172.16.0.255 scope global eth0
       valid_lft forever preferred_lft forever
       
# server03测试服务
[root@server03 ~]# curl localhost
my is keepalived server01-172.16.0.1

# 停止server01的keepalived
[root@server01 ~]# systemctl stop keepalived

# 再次测试
[root@server03 ~]# curl localhost
my is keepalived server02-172.16.0.2
```

> 经过测试，跟本地keepalived功能一样，只是HaVip在公测阶段，不太稳定，慎用
>
> 在HaVip控制台中删除绑定ECS的server02，此时VIP还是绑定在此机器 ，只有当VIP经过飘移到其它机器时后，此VIP绑定ECS才失效。





















## 其它

```bash
# mysqld_exporter
mysql> grant select,process,replication client on *.* to 'mysqld_exporter'@'10.10.10.%' identified by 'lVPl5nwV8GriDhcP';

# mysql最大可用内存
select (@@key_buffer_size +@@innodb_buffer_pool_size + @@tmp_table_size + @@max_connections*(@@read_buffer_size + @@read_rnd_buffer_size + @@sort_buffer_size + @@join_buffer_size + @@binlog_cache_size + @@thread_stack) )/1024/1024 as "Total_AllMem result";

# 阿里云知识点
redis、mysql数据库都需要设置白名单才可连接使用，其中redis连接密码为阿里云控制台设置的<user>:<password>，才可以用redis-cli等客户端连接

# 阿里云elasticsearch备份恢复整个快照
POST _snapshot/aliyun_auto_snapshot/<snapshot>/_restore?wait_for_completion=true
#elasticsearch备份恢复快照中的指定索引
POST _snapshot/aliyun_auto_snapshot/<snapshot>/_restore
{
"indices": "index_1",
"rename_pattern": "index_(.+)",
"rename_replacement": "restored_index_$1"
}


# 问题汇总

# Redis相关问题
程序读写除DB0以外的数据库时不成功，问题在哪？
答：如果您的Redis实例为集群架构或读写分离架构，且需要执行切换或选择数据库的操作（即使用多数据库功能），您必须先将cluster_compat_enable参数设置为0（即关闭原生Redis Cluster语法兼容），然后重启客户端应用。


# ansible笔记
[root@jumpserver ansible]# ansible all -m copy -a 'src=/etc/sysconfig/network-scripts/ifcfg-eth0 dest=/etc/sysconfig/network-scripts/ifcfg-eth0 owner=root group=root mode=644 backup=yes'
[root@jumpserver ansible]# ansible all -m shell -a 'cat /etc/sysconfig/network-scripts/ifcfg-eth0'
[root@jumpserver ansible]# ansible all -m shell -a 'systemctl restart network'
[root@jumpserver ansible]# ansible all -m shell -a 'ping -qA -s 500 -w 1000 -c 10 hs.com'
```

​	
